{"meta":{"title":"非鱼博客","subtitle":"非鱼博客小站","description":"","author":"烦恼多一点","url":"http://example.com","root":"/"},"pages":[{"title":"文章归档","date":"2022-06-29T03:23:46.887Z","updated":"2022-06-29T03:23:46.887Z","comments":true,"path":"archive.html","permalink":"http://example.com/archive.html","excerpt":"","text":""},{"title":"links","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:24:11.052Z","comments":true,"path":"PY.html","permalink":"http://example.com/PY.html","excerpt":"","text":""}],"posts":[{"title":"JDK8 新特性","slug":"1.Java 基础 - JDK8 新特性","date":"2022-07-13T12:46:25.000Z","updated":"2022-06-28T03:29:09.954Z","comments":true,"path":"2022/07/13/1.Java 基础 - JDK8 新特性/","link":"","permalink":"http://example.com/2022/07/13/1.Java%20%E5%9F%BA%E7%A1%80%20-%20JDK8%20%E6%96%B0%E7%89%B9%E6%80%A7/","excerpt":"","text":"Java 基础 - JDK8 新特性概述以下列出两点重要特性： Lambda表达式（匿名函数） Stream多线程并行数据处理（重要）新特性 接口的默认方法只需要使用default关键字即可，这个特性又叫扩展方法 Lambda表达式 Functional接口函数式接口是指仅仅只包含一个抽象方法的接口，每一个该类型的Lambda表达式都会被匹配到这个抽象方法。你只需要给你的接口添加@FunctionalInterface注解。 使用::双冒号关键字来传递方法（静态方法和非静态方法） Predicate接口和Lambda表达式 Function接口 Function有一个参数并且返回一个结果，并附带了一些可以和其他函数组合的默认方法。 compose方法表示在某个方法之前执行 andThen方法表示在某个方法之后执行 注意：compose和andThen方法调用之后都会把对象自己本身返回，这可以 方便链式编程 Supplier接口，返回一个任意泛型的值，和Function接口不同的是该接口 没有任何参数 Consumer接口，接收一个任意泛型的值，和Function接口不同的是该接口 没有任何值 Optional类 Optional不是接口而是一个类，这是个用来防止NullPointerException异常的辅助类型 Optional被定义为一个简单的容器，其值可能是null或者不是null 在Java8之前一般某个函数应该返回非空对象但是偶尔却可能返回了null，而在Java8中不推荐你返回 null 而是返回Optional。 这是一个可以为 null 的容器对象。 如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。小栗子1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package com.funtl.jdk8.feature.lambda;import java.util.Arrays;import java.util.List;import java.util.stream.Collectors;/** * Lambda 基本用法 * &lt;p&gt;Title: BaseLambda&lt;/p&gt; * &lt;p&gt;Description: &lt;/p&gt; * * @author Lusifer * @version 1.0.0 * @date 2019/1/6 10:42 */public class BaseLambda &#123; public static void main(String[] args) &#123; testForeach(); testStreamDuplicates(); &#125; /** * Lambda 遍历 */ public static void testForeach() &#123; // 定义一个数组 String[] array = &#123; &quot;尼尔机械纪元&quot;, &quot;关于我转生成为史莱姆这件事&quot;, &quot;实力至上主义教师&quot;, &quot;地狱少女&quot; &#125;; // 转换成集合 List&lt;String&gt; acgs = Arrays.asList(array); // 传统的遍历方式 System.out.println(&quot;传统的遍历方式：&quot;); for (String acg : acgs) &#123; System.out.println(acg); &#125; System.out.println(); // 使用 Lambda 表达式以及函数操作(functional operation) System.out.println(&quot;Lambda 表达式以及函数操作：&quot;); acgs.forEach((acg) -&gt; System.out.println(acg)); System.out.println(); // 在 Java 8 中使用双冒号操作符(double colon operator) System.out.println(&quot;使用双冒号操作符：&quot;); acgs.forEach(System.out::println); System.out.println(); &#125; /** * Stream 去重复 * String 和 Integer 可以使用该方法去重 */ public static void testStreamDuplicates() &#123; System.out.println(&quot;Stream 去重复：&quot;); // 定义一个数组 String[] array = &#123; &quot;尼尔机械纪元&quot;, &quot;尼尔机械纪元&quot;, &quot;关于我转生成为史莱姆这件事&quot;, &quot;关于我转生成为史莱姆这件事&quot;, &quot;实力至上主义教师&quot;, &quot;实力至上主义教师&quot;, &quot;地狱少女&quot;, &quot;地狱少女&quot; &#125;; // 转换成集合 List&lt;String&gt; acgs = Arrays.asList(array); // Stream 去重复 acgs = acgs.stream().distinct().collect(Collectors.toList()); // 打印 acgs.forEach(System.out::println); &#125;&#125;","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"双亲委派模型","slug":"2.Java 基础 - 双亲委派模型","date":"2022-07-13T12:46:25.000Z","updated":"2022-06-28T09:52:45.384Z","comments":true,"path":"2022/07/13/2.Java 基础 - 双亲委派模型/","link":"","permalink":"http://example.com/2022/07/13/2.Java%20%E5%9F%BA%E7%A1%80%20-%20%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"类加载器加载类的开放性类加载器（ClassLoader) 是Java语言的一项创新，也是Java语言流行的一个重要原因，在类加载的第一阶段“加载”过程中需要通过一个类的全限定名来回去定义此类的二进制字节流，完成这个动作的代码块就是 类加载器，这一动作是放在Java虚拟机外部去实现的，以便让应用程序自己去决定如何获取所需的类。 虚拟机规范并没有指明二进制字节流要从一个Class文件获取，或者说根本没有指明从哪里获取、怎样获取。这种开放使得Java在很多领域得到充分运用，例如： 从ZIP包中读取，这很常见，成为JAR,EAR,WAR格式的基础 从网络中获取，最典型的运用就是Applet 运用时计算生成，最典型的是动态代理技术，在java.lang.reflect.Proxy中，就是用了ProxyGrenerator.generateProxyClass 来为特定接口生成形式为“*$Proxy”的代理类的二进制字节流 有其它文件生成，最典型的JSP应用，由JSP文件生成对应的Class类 类加载器与类的唯一性类加载器虽然只用于实现类的加载动作，但是对于任意一个类，都需要由它的类加载器和这个类本身共同确立其在Java虚拟机中的唯一性。通俗的说JVM中两个类是否“相等”，首先就必须是同一个类加载器加载的，否则，即使这两个类加来源于同一个Class文件，被同一个虚拟机加载，只要类加载器不同那么这两个类必定是不相同的。 第二部分使用自定义的类加载器加载ClassLoaderTest,classcom.jvm.classloading.ClassLoaderTest显示，obj2确实是类com.jvm.classloading.ClassLoaderTest实例化出来的对象，但是第二句输出false此时虚拟机中有3个ClassLoaderTest类，由于第3个类的类加载器与前面2个类加载器不同，虽然来源于同一个Class文件，但是它是一个独立的类，所属类型检查是返回结果自然是false。 双亲委派模型类加载器种类从Java虚拟机的角度来说，只存在两种不同的类加载器：一种是启动类加载器（BootstrapClassLoader),这种类加载器使用C++语言实现（HotSport虚拟机中），是虚拟机自身的一部分；另一种就是所有其他的类加载器，这些类加载器都有Java语言实现，独立于虚拟机外部，并且全部继承自java.lang.ClassLoader。从开发者的角度，类加载器可以细分为： 启动（Bootstrap)类加载器：负责将java_Home&#x2F;lib下面的类库加载到内存中（比如rt.jar)。由于引导类加载器涉及到虚拟机本地实现细节开发者无法直接获取到启动类的加载器的引用，所以不允许直接通过应用进行操作。 标准扩展（Extension)类加载器：是由Sun的**ExtClassLoader(sun.misc.Launcher$ExtClassLoader)**实现的。它负责将java_home&#x2F;lib&#x2F;ext或者由系统变量Java.ext.dir指定位置中的类库加载到内存中。开发者可以直接使用标准扩展类加载器。 应用程序（Application)类加载器：是由Sun的AppClassLoader(sun.misc.Launcher$AppClassLoader) 实现的。它负责将系统类路径（CLASSPATH)中指定的类库加载到内存中。开发者可以直接使用系统类加载器。由于这个类加载器是ClassLoader中getSystemClassLoader()方法的返回值，因此一般称为（System）加载器。除此之外， 还有自定义的类加载器，他们之间的层次关系被称为类加载器的双亲委派模型。该模型要求除了顶层的启动类加载器外，其他的类加载器都应该有自己的父类加载器，而这种父子关系一般通过组合（Composition）关系；来实现，而不是通过继承（Inheritance)。 双亲委派模型双亲委派模型过程某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父类加载器，依次递归，如果父类加载器可以完成加载任务，就成功返回；只有父类加载器无法完成此加载任务时，才自己去加载。使用双亲委派模型的好处在于Java类随着他的类加载器一起具备了一种带有优先级的层次关系 。例如类java.lang.Object ,它存在在rt.jar 中，无论哪一个类加载器要加载这个类，最终都是委派给处于模型最顶端的Bootstrap ClassLoader 进行加载，因此Object类在程序的各种类加载器环境中都是同一个类。相反，如果没有双亲委派模型而是由各个类加载器自行加载的话，如果用户编写了一个java.lang.Object 的同名并放在ClassPath中，那系统中将会出现很多个不同的Object类，程序将混乱。因此，如果开发者尝试编写一个与rt.jar类库中重名的Java类，可以正常编译，但是永远无法被加载运动 双亲委派模型的系统实现在java.lang.ClassLoader 的loadClass() 方法中，先检查是否已经被加载过，若没有加载则调用父类加载器的loadClass()方法，若父加载器为空则默认使用启动类加载器作为父加载器。如果父加载失败，则抛出ClassNotFounction 异常后，再调用自己的findClass() 方法进行加载。 12345678910111213141516171819202122protected synchronized Class&lt;?&gt; loadClass(String name,boolean resolve)throws ClassNotFoundException&#123; //check the class has been loaded or not Class c = findLoadedClass(name); if(c == null)&#123; try&#123; if(parent != null)&#123; c = parent.loadClass(name,false); &#125;else&#123; c = findBootstrapClassOrNull(name); &#125; &#125;catch(ClassNotFoundException e)&#123; //if throws the exception ,the father can not complete the load &#125; if(c == null)&#123; c = findClass(name); &#125; &#125; if(resolve)&#123; resolveClass(c); &#125; return c;&#125; 注意，双亲委派模型是 Java 设计者推荐给开发者的类加载器的实现方式，并不是强制规定的。大多数的类加载器都遵循这个模型，但是 JDK 中也有较大规模破坏双亲模型的情况，例如线程上下文类加载器（Thread Context ClassLoader）的出现","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"List 和 Map 区别","slug":"20.Java 集合 - List 和 Map 区别","date":"2022-07-13T12:46:25.000Z","updated":"2022-06-29T03:02:57.827Z","comments":true,"path":"2022/07/13/20.Java 集合 - List 和 Map 区别/","link":"","permalink":"http://example.com/2022/07/13/20.Java%20%E9%9B%86%E5%90%88%20-%20List%20%E5%92%8C%20Map%20%E5%8C%BA%E5%88%AB/","excerpt":"","text":"List 特点：元素有放入顺序，元素可重复; Map 特点：元素按键值对存储，无放入顺序 ; List 接口有三个实现类：LinkedList，ArrayList，Vector; LinkedList：底层基于链表实现，链表内存是散乱的，每一个元素存储本身内存地址的同时还存储下一个元素的地址。链表增删快，查找慢; Map 接口有三个实现类：HashMap，HashTable，LinkedHashMap Map 相当于和 Collection 一个级别的；Map 集合存储键值对，且要求保持键的唯一性；","categories":[],"tags":[]},{"title":"死信、延迟、重试队列","slug":"3.消息队列 - 死信、延迟、重试队列","date":"2022-07-13T12:46:25.000Z","updated":"2022-06-28T09:48:29.229Z","comments":true,"path":"2022/07/13/3.消息队列 - 死信、延迟、重试队列/","link":"","permalink":"http://example.com/2022/07/13/3.%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%20-%20%E6%AD%BB%E4%BF%A1%E3%80%81%E5%BB%B6%E8%BF%9F%E3%80%81%E9%87%8D%E8%AF%95%E9%98%9F%E5%88%97/","excerpt":"","text":"死信队列DLQ(Deal Letter Queue)，死信队列。当一个消息在队列中变成死信之后，他能被重新发送到 DLQ 中，与 DLQ 绑定到队列就是死信队列。 什么情况下需要死信队列 消息被拒绝 消息过期 队列达到最大长度 生产者生产一条消息，存储到普通队列中；设置队列的过期时间为 10 秒，在 10 秒内没有消费者消费消息，那么判定消息过期；此时如果设置了死信队列，过期消息被丢给死信队列交换机，然后被存储在死信队列中。 延迟队列顾名思义就是延迟执行消息，比如我们可以增加一个队列并设置其超时时间为 10 秒并且不设置任何消费者，等到消息超时，我们可以将消息放入死信队列，让消费者监听这个死信队列就达到了延迟队列的效果。 重试队列重试的消息在延迟的某个时间点（业务可设置）后，再次投递给消费者。而如果一直这样重复消费都持续失败到一定次数，就会投递到死信队列，最后需要进行人工干预。","categories":[],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"http://example.com/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]},{"title":"Zookeeper-假死闹裂","slug":"4.Zookeeper - 假死脑裂","date":"2022-07-13T12:46:25.000Z","updated":"2022-06-28T09:48:29.229Z","comments":true,"path":"2022/07/13/4.Zookeeper - 假死脑裂/","link":"","permalink":"http://example.com/2022/07/13/4.Zookeeper%20-%20%E5%81%87%E6%AD%BB%E8%84%91%E8%A3%82/","excerpt":"","text":"该问题就是服务集群因为网络震荡导致的多主多从问题，解决方案就是设置服务切换的超时时间，但也同时会导致无法达到高可用的要求。 注：该问题没什么卵用，就是为了告诉大家有 “假死脑裂” 这个词，避免面试时尴尬 ╮(￣▽￣)╭","categories":[],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://example.com/tags/Zookeeper/"}]},{"title":"MySQL-优化","slug":"5.MySQL - 优化（重点掌握）","date":"2022-07-13T12:46:25.000Z","updated":"2022-06-28T09:48:29.229Z","comments":true,"path":"2022/07/13/5.MySQL - 优化（重点掌握）/","link":"","permalink":"http://example.com/2022/07/13/5.MySQL%20-%20%E4%BC%98%E5%8C%96%EF%BC%88%E9%87%8D%E7%82%B9%E6%8E%8C%E6%8F%A1%EF%BC%89/","excerpt":"","text":"概述 表关联查询时务必遵循 小表驱动大表 原则； 使用查询语句 where 条件时，不允许出现 函数，否则索引会失效； 使用单表查询时，相同字段尽量不要用 OR，因为可能导致索引失效，比如：SELECT * FROM table WHERE name = &#39;手机&#39; OR name = &#39;电脑&#39;，可以使用 UNION 替代； LIKE 语句不允许使用 % 开头，否则索引会失效； 组合索引一定要遵循 从左到右 原则，否则索引会失效；比如：SELECT * FROM table WHERE name = &#39;张三&#39; AND age = 18，那么该组合索引必须是 name,age 形式； 索引不宜过多，根据实际情况决定，尽量不要超过 10 个； 每张表都必须有 主键，达到加快查询效率的目的； 分表，可根据业务字段尾数中的个位或十位或百位（以此类推）做表名达到分表的目的； 分库，可根据业务字段尾数中的个位或十位或百位（以此类推）做库名达到分库的目的； 表分区，类似于硬盘分区，可以将某个时间段的数据放在分区里，加快查询速度，可以配合 分表 + 表分区 结合使用； 神器 EXPLAIN 语句EXPLAIN 显示了 MySQL 如何使用索引来处理 SELECT 语句以及连接表。可以帮助选择更好的索引和写出更优化的查询语句。 使用方法，在 SELECT 语句前加上 EXPLAIN 即可，如： 1EXPLAIN SELECT * FROM tb_item WHERE cid IN (SELECT id FROM tb_item_cat) id： SELECT 识别符。这是 SELECT 的查询序列号 select_type： SELECT类型,可以为以下任何一种 SIMPLE: 简单 SELECT(不使用 UNION 或子查询) PRIMARY: 最外面的 SELECT UNION: UNION 中的第二个或后面的 SELECT 语句 DEPENDENT UNION: UNION 中的第二个或后面的 SELECT 语句,取决于外面的查询 UNION RESULT: UNION 的结果 SUBQUERY: 子查询中的第一个 SELECT DEPENDENT SUBQUERY: 子查询中的第一个 SELECT,取决于外面的查询 DERIVED: 导出表的 SELECT(FROM 子句的子查询) table： 输出的行所引用的表 partitions： 表分区 type： 联接类型。下面给出各种联接类型，按照 从最佳类型到最坏类型 进行排序 system: 表仅有一行(&#x3D;系统表)。这是 const 联接类型的一个特例。 const: 表最多有一个匹配行,它将在查询开始时被读取。因为仅有一行,在这行的列值可被优化器剩余部分认为是常数。const 表很快,因为它们只读取一次! eq_ref: 对于每个来自于前面的表的行组合, 从该表中读取一行。这可能是最好的联接类型, 除了 const 类型。 ref: 对于每个来自于前面的表的行组合, 所有有匹配索引值的行将从这张表中读取。 ref_or_null: 该联接类型如同 ref,但是添加了 MySQL 可以专门搜索包含 NULL 值的行。 index_merge: 该联接类型表示使用了索引合并优化方法。 unique_subquery: 该类型替换了下面形式的 IN 子查询的 ref: value IN (SELECT primary_key FROM single_table WHERE some_expr) unique_subquery 是一个索引查找函数, 可以完全替换子查询, 效率更高。 index_subquery: 该联接类型类似于 unique_subquery。可以替换 IN 子查询, 但只适合下列形式的子查询中的非唯一索引: value IN (SELECT key_column FROM single_table WHERE some_expr) range: 只检索给定范围的行,使用一个索引来选择行。 index: 该联接类型与 ALL 相同,除了只有索引树被扫描。这通常比 ALL 快,因为索引文件通常比数据文件小。 ALL: 对于每个来自于先前的表的行组合, 进行完整的表扫描。 possible_keys： 指出 MySQL 能使用哪个索引在该表中找到行 key： 显示 MySQL 实际决定使用的键(索引)。如果没有选择索引, 键是 NULL。 key_len： 显示 MySQL 决定使用的键长度。如果键是 NULL, 则长度为 NULL。 ref： 显示使用哪个列或常数与 key 一起从表中选择行。 rows： 显示 MySQL 认为它执行查询时必须检查的行数。多行之间的数据相乘可以估算要处理的行数。 filtered： 显示了通过条件过滤出的行数的百分比估计值。 Extra： 该列包含 MySQL 解决查询的详细信息 Distinct: MySQL 发现第 1 个匹配行后,停止为当前的行组合搜索更多的行。 Not exists: MySQL 能够对查询进行 LEFT JOIN 优化, 发现 1 个匹配 LEFT JOIN 标准的行后, 不再为前面的的行组合在该表内检查更多的行。 range checked for each record (index map: #): MySQL 没有发现好的可以使用的索引, 但发现如果来自前面的表的列值已知, 可能部分索引可以使用。 Using filesort: MySQL 需要额外的一次传递, 以找出如何按排序顺序检索行。 Using index: 从只使用索引树中的信息而不需要进一步搜索读取实际的行来检索表中的列信息。 Using temporary: 为了解决查询, MySQL 需要创建一个临时表来容纳结果。 Using where: WHERE 子句用于限制哪一个行匹配下一个表或发送到客户。 Using sort_union(…), Using union(…), Using intersect(…): 这些函数说明如何为 index_merge 联接类型合并索引扫描。 Using index for group-by: 类似于访问表的 Using index 方式,Using index for group-by 表示 MySQL 发现了一个索引,可以用来查询 GROUP BY 或 DISTINCT 查询的所有列, 而不要额外搜索硬盘访问实际的表。","categories":[],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"}]},{"title":"面向对象的特征","slug":"6.Java 基础 - 面向对象的特征","date":"2022-07-13T12:46:25.000Z","updated":"2022-06-28T09:48:29.230Z","comments":true,"path":"2022/07/13/6.Java 基础 - 面向对象的特征/","link":"","permalink":"http://example.com/2022/07/13/6.Java%20%E5%9F%BA%E7%A1%80%20-%20%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E7%89%B9%E5%BE%81/","excerpt":"","text":"概述面向对象的三个基本特征是：封装、继承、多态。 封装封装最好理解了。封装是面向对象的特征之一，是对象和类概念的主要特性。封装，也就是把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。 继承面向对象编程 (OOP) 语言的一个主要功能就是“继承”。继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。 多态多态性（polymorphisn）是允许你将父对象设置成为和一个或更多的他的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。简单的说，就是一句话：允许将子类类型的指针赋值给父类类型的指针。 实现多态，有二种方式，覆盖，重载。","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"final，finally,finalize","slug":"7.Java 基础 - final, finally, finalize","date":"2022-07-13T12:46:25.000Z","updated":"2022-06-28T09:52:45.384Z","comments":true,"path":"2022/07/13/7.Java 基础 - final, finally, finalize/","link":"","permalink":"http://example.com/2022/07/13/7.Java%20%E5%9F%BA%E7%A1%80%20-%20final,%20finally,%20finalize/","excerpt":"","text":"final用于声明属性,方法和类, 分别表示属性不可变, 方法不可覆盖, 类不可继承. finally是异常处理语句结构的一部分，表示总是执行. finalize是Object类的一个方法，在垃圾收集器执行的时候会调用被回收对象的此方法，可以覆盖此方法提供垃圾收集时的其他资源回收，例如关闭文件等. JVM不保证此方法总被调用.","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"MySQL规范","slug":"83.数据存储 - MySQL 规范","date":"2022-07-13T12:46:25.000Z","updated":"2022-06-24T06:23:32.893Z","comments":true,"path":"2022/07/13/83.数据存储 - MySQL 规范/","link":"","permalink":"http://example.com/2022/07/13/83.%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%20-%20MySQL%20%E8%A7%84%E8%8C%83/","excerpt":"","text":"基础规范 表存储引擎必须使用InnoDB 表字符集默认使用utf8，必要时使用·utf8bm4 通用无乱码分险，汉字3字节，英文1字节 utf8mb4是utf8的超集，有存储4字节例如表情符号时，使用它 禁止使用存储过程，视图，触发器，Event 对数据库性能影响较大，互联网业务，能让站点层和服务层干的事情，不要交到数据库层 调试，排错，迁移都比较困难，扩展性较差 禁止在数据库中存储发文件，例如图片，可以将大文件存储在对象存储系统，数据库中存储路径 禁止在线上环境做数据库压力测试 测试，开发，线上数据库环境必须隔离 命名规范 库名，表名，列名必须使用小写，采用下划线分隔 abc,Abc,ABC都是给自己埋坑 库名，表名，列名必须见名知意，长度不要超过32字符 tmp,wushan谁TM知道这些库是干嘛的 库备份必须以bak为前缀，以日期为后缀 从库必须以-s为后缀 备库必须以-ss为后缀 表设计规范 单实例表个数必须控制在 2000 个以内 单表分表个数必须控制在 1024 个以内 表必须有主键，推荐使用 UNSIGNED整数为主键 删除无主键的表，如果是 row 模式的主从架构，从库会挂住 禁止使用外键，如果要保证完整性，应由应用程式实现 外键使得表之间相互耦合，影响 update/delete 等 SQL 性能，有可能造成死锁，高并发情况下容易成为数据库瓶颈 建议将大字段，访问频度低的字段拆分到单独的表中存储，分离冷热数据（具体参考：《如何实施数据库垂直拆分》） 列设计规范 根据业务区分使用 tinyint/int/bigint，分别会占用 1/4/8 字节 根据业务区分使用 char/varchar 字段长度固定，或者长度近似的业务场景，适合使用 char，能够减少碎片，查询性能高 字段长度相差较大，或者更新较少的业务场景，适合使用 varchar，能够减少空间 根据业务区分使用 datetime/timestamp 前者占用 5 个字节，后者占用 4 个字节，存储年使用 YEAR，存储日期使用 DATE，存储时间使用 datetime 必须把字段定义为 NOT NULL 并设默认值 NULL 的列使用索引，索引统计，值都更加复杂，MySQL 更难优化 NULL 需要更多的存储空间 NULL 只能采用 IS NULL 或者 IS NOT NULL ，而在 =/!=/in/not in 时有大坑 使用 INT UNSIGNED 存储 IPv4 ，不要用 char(15) 使用 varchar(20)存储手机号，不要使用整数 牵扯到国家代号，可能出现 +/-/() 等字符，例如 +86 手机号不会用来做数学运算 varchar 可以模糊查询，例如 like‘138%’ 使用 TINYINT来代替 ENUM ENUM 增加新值要进行 DDL 操作 索引规范 唯一索引使用 uniq_[字段名] 来命名 非唯一索引使用 idx_[字段名] 来命名 单张表索引数量建议控制在 5 个以内 互联网高并发业务，太多索引会影响写性能 生成执行计划时，如果索引太多，会降低性能，并可能导致 MySQL 选择不到最优索引 异常复杂的查询需求，可以选择 ES 等更为适合的方式存储 组合索引字段数不建议超过 5 个 如果 5 个字段还不能极大缩小 row 范围，八成是设计有问题 不建议在频繁更新的字段上建立索引 非必要不要进行 JOIN 查询，如果要进行 JOIN查询，被 JOIN的字段必须类型相同，并建立索引 踩过因为 JOIN 字段类型不一致，而导致全表扫描的坑么？ 理解组合索引最左前缀原则，避免重复建设索引，如果建立了(a,b,c)，相当于建立了(a), (a,b), (a,b,c) SQL 规范 禁止使用 select *，只获取必要字段 select * 会增加 cpu/io/内存/带宽 的消耗 指定字段能有效利用索引覆盖 指定字段查询，在表结构变更时，能保证对应用程序无影响 insert必须指定字段，禁止使用 insert into T values() 指定字段插入，在表结构变更时，能保证对应用程序无影响 隐式类型转换会使索引失效，导致全表扫描 禁止在 where 条件列使用函数或者表达式 导致不能命中索引，全表扫描 禁止负向查询以及 % 开头的模糊查询 导致不能命中索引，全表扫描 禁止大表 JOIN 和子查询 同一个字段上的 OR 必须改写问 IN，IN 的值必须少于 50 个 应用程序必须捕获 SQL 异常 方便定位线上问题 说明本规范适用于并发量大，数据量大的典型互联网业务，可直接带走参考，不谢。","categories":[],"tags":[]},{"title":"mysql索引使用的注意事项","slug":"84数据存储 - MySQL 索引使用的注意事项","date":"2022-07-13T12:46:25.000Z","updated":"2022-06-24T06:23:34.238Z","comments":true,"path":"2022/07/13/84数据存储 - MySQL 索引使用的注意事项/","link":"","permalink":"http://example.com/2022/07/13/84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%20-%20MySQL%20%E7%B4%A2%E5%BC%95%E4%BD%BF%E7%94%A8%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/","excerpt":"","text":"索引不会包含有NULL的列 只要列中包含有NULL 值，都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。 使用短索引 对串列进行索引，如果可以就应该指定一个前缀长度。例如，如果有一个 char（255） 的列，如果在前 10 个或 20 个字符内，多数值是唯一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和 I/O 操作。 索引列排序 MySql 查询只使用一个索引，因此如果 where 子句中已经使用了索引的话，那么 order by 中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作，尽量不要包含多个列的排序，如果需要最好给这些列建复合索引。 like 语句操作 一般情况下不鼓励使用 like 操作，如果非使用不可，注意正确的使用方式。like ‘%aaa%’ 不会使用索引，而 like ‘aaa%’ 可以使用索引。 不要在列上进行运算 不使用 NOT IN 、&lt;&gt;、！=操作，但 &lt; , &lt;= ，= ，&gt; , &gt;= , BETWEEN , IN 是可以用到索引的 索引要建立在经常进行select操作的字段上 这是因为，如果这些列很少用到，那么有无索引并不能明显改变查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 索引要建立在值比较唯一的字段上 对于那些定义为 text、image 和 bit 数据类型的列不应该增加索引。因为这些列的数据量要么相当大，要么取值很少 在 where 和 join 中出现的列需要建立索引 where 的查询条件里有不等号 (where column != …) , MySql 将无法使用索引 如果 where 字句的查询条件里使用了函数(如：where DAY(column)=…), MySql 将无法使用索引 在 join 操作中(需要从多个数据表提取数据时)，MySql 只有在主键和外键的数据类型相同时才能使用索引，否则及时建立了索引也不会使用","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2022-06-28T03:01:01.932Z","updated":"2022-06-02T01:59:15.706Z","comments":true,"path":"2022/06/28/hello-world/","link":"","permalink":"http://example.com/2022/06/28/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"瀑布流图片","slug":"第一篇Hexo-blog文章","date":"2022-06-02T02:05:36.000Z","updated":"2022-06-17T08:03:05.110Z","comments":true,"path":"2022/06/02/第一篇Hexo-blog文章/","link":"","permalink":"http://example.com/2022/06/02/%E7%AC%AC%E4%B8%80%E7%AF%87Hexo-blog%E6%96%87%E7%AB%A0/","excerpt":"","text":"","categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"}]},{"title":"抽象类和接口有什么区别","slug":"10.Java 基础 - 抽象类和接口有什么区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-28T09:52:45.384Z","comments":true,"path":"2013/07/13/10.Java 基础 - 抽象类和接口有什么区别/","link":"","permalink":"http://example.com/2013/07/13/10.Java%20%E5%9F%BA%E7%A1%80%20-%20%E6%8A%BD%E8%B1%A1%E7%B1%BB%E5%92%8C%E6%8E%A5%E5%8F%A3%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/","excerpt":"","text":"参数 抽象类 接口 默认的方法实现 它可以有默认的方法实现 接口完全是抽象的。它根本不存在方法的实现 实现 子类使用 extends 关键字来继承抽象类。如果子类不是抽象类的话，它需要提供抽象类中所有声明的方法的实现。 子类使用关键字 implements 来实现接口。它需要提供接口中所有声明的方法的实现 构造器 抽象类可以有构造器 接口不能有构造器 与正常 Java 类的区别 除了你不能实例化抽象类之外，它和普通Java类没有任何区别 接口是完全不同的类型 访问修饰符 抽象方法可以有 public、protected 和 default 这些修饰符 接口方法默认修饰符是 public。你不可以使用其它修饰符。 main 方法 抽象方法可以有 main 方法并且我们可以运行它 接口没有 main 方法，因此我们不能运行它。 多继承 抽象方法可以继承一个类和实现多个接口 接口只可以继承一个或多个其它接口 速度 它比接口速度要快 接口是稍微有点慢的，因为它需要时间去寻找在类中实现的方法。 添加新方法 如果你往抽象类中添加新的方法，你可以给它提供默认的实现。因此你不需要改变你现在的代码。 如果你往接口中添加方法，那么你必须改变实现该接口的类。","categories":[],"tags":[]},{"title":"说说自定义注解的场景及实现","slug":"12.Java 基础 - 说说自定义注解的场景及实现","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-28T09:54:18.509Z","comments":true,"path":"2013/07/13/12.Java 基础 - 说说自定义注解的场景及实现/","link":"","permalink":"http://example.com/2013/07/13/12.Java%20%E5%9F%BA%E7%A1%80%20-%20%E8%AF%B4%E8%AF%B4%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%E7%9A%84%E5%9C%BA%E6%99%AF%E5%8F%8A%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"登陆、权限拦截、日志处理，以及各种 Java 框架，如 Spring，Hibernate，JUnit 提到注解就不能不说反射，Java 自定义注解是通过运行时靠反射获取注解。实际开发中，例如我们要获取某个方法的调用日志，可以通过 AOP（动态代理机制）给方法添加切面，通过反射来获取方法包含的注解，如果包含日志注解，就进行日志记录。反射的实现在 Java 应用层面上讲，是通过对 Class 对象的操作实现的，Class 对象为我们提供了一系列方法对类进行操作。在 JVM 这个角度来说，Class 文件是一组以 8 位字节为基础单位的二进制流，各个数据项目按严格的顺序紧凑的排列在 Class 文件中，里面包含了类、方法、字段等等相关数据。通过对 Class 数据流的处理我们即可得到字段、方法等数据。","categories":[],"tags":[]},{"title":"Session 与 Cookie 区别","slug":"14.Java 基础 - Session 与 Cookie 区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-28T09:54:15.072Z","comments":true,"path":"2013/07/13/14.Java 基础 - Session 与 Cookie 区别/","link":"","permalink":"http://example.com/2013/07/13/14.Java%20%E5%9F%BA%E7%A1%80%20-%20Session%20%E4%B8%8E%20Cookie%20%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Cookie 数据存放在客户的浏览器上，Session 数据放在服务器上。 Cookie 不是很安全，别人可以分析存放在本地的 Cookie 并进行 Cookie 欺骗，考虑到安全应当使用 Session。 Session 会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，考虑到减轻服务器性能方面，应当使用 Cookie。 单个 Cookie 保存的数据不能超过 4K，很多浏览器都限制一个站点最多保存 20 个 Cookie。","categories":[],"tags":[]},{"title":"HTTP 请求的 GET 与 POST 方式的区别","slug":"13.Java 基础 - HTTP 请求的 GET 与 POST 方式的区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-28T09:54:18.509Z","comments":true,"path":"2013/07/13/13.Java 基础 - HTTP 请求的 GET 与 POST 方式的区别/","link":"","permalink":"http://example.com/2013/07/13/13.Java%20%E5%9F%BA%E7%A1%80%20-%20HTTP%20%E8%AF%B7%E6%B1%82%E7%9A%84%20GET%20%E4%B8%8E%20POST%20%E6%96%B9%E5%BC%8F%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"根据 HTTP 规范，GET 用于信息获取，而且应该是安全的和幂等的。 根据 HTTP 规范，POST 表示可能修改变服务器上的资源的请求。 首先是 “GET 方式提交的数据最多只能是 1024 字节”，因为 GET 是通过 URL 提交数据，那么 GET 可提交的数据量就跟 URL 的长度有直接关系了。而实际上，URL 不存在参数上限的问题，HTTP 协议规范没有对 URL 长度进行限制。这个限制是特定的浏览器及服务器对它的限制。IE 对 URL 长度的限制是 2083 字节(2K+35)。对于其他浏览器，如 Netscape、FireFox 等，理论上没有长度限制，其限制取决于操作系统的支持。注意这是限制是整个 URL 长度，而不仅仅是你的参数值数据长度。 POST 是没有大小限制的，HTTP 协议规范也没有进行大小限制 以上为标准答案，实际回答时可增加如下两条： 首先是语义区别，GET 为获取，POST 为提交；（分清语义区别是为了更好的实现 RESTFul 风格 API） 其次是 GET 请求只请求服务器一次，但 POST 会请求两次，第一次是 OPTIONS 方式请求为了确定服务器是否能够接收数据，第二次才是真的 POST 请求，将数据提交到服务器；","categories":[],"tags":[]},{"title":"JDBC 流程","slug":"16.Java 基础 - JDBC 流程","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T01:46:59.833Z","comments":true,"path":"2013/07/13/16.Java 基础 - JDBC 流程/","link":"","permalink":"http://example.com/2013/07/13/16.Java%20%E5%9F%BA%E7%A1%80%20-%20JDBC%20%E6%B5%81%E7%A8%8B/","excerpt":"","text":"向 DriverManager 类注册驱动数据库驱动程序 调用 DriverManager.getConnection 方法， 通过 JDBC URL，用户名，密码取得数据库连接的 Connection 对象。 获取 Connection 后， 便可以通过 createStatement 创建 Statement 用以执行 SQL 语句。 有时候会得到查询结果，比如 select，得到查询结果，查询（SELECT）的结果存放于结果集（ResultSet）中。 关闭数据库语句，关闭数据库连接。","categories":[],"tags":[]},{"title":"Session 分布式处理","slug":"15.Java 基础 - Session 分布式处理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T01:46:59.833Z","comments":true,"path":"2013/07/13/15.Java 基础 - Session 分布式处理/","link":"","permalink":"http://example.com/2013/07/13/15.Java%20%E5%9F%BA%E7%A1%80%20-%20Session%20%E5%88%86%E5%B8%83%E5%BC%8F%E5%A4%84%E7%90%86/","excerpt":"","text":"Session 复制在支持 Session 复制的 Web 服务器上，通过修改 Web 服务器的配置，可以实现将 Session 同步到其它 Web 服务器上，达到每个 Web 服务器上都保存一致的 Session。 优点：代码上不需要做支持和修改。 缺点：需要依赖支持的 Web 服务器，一旦更换成不支持的 Web 服务器就不能使用了，在数据量很大的情况下不仅占用网络资源，而且会导致延迟。 适用场景：只适用于Web服务器比较少且 Session 数据量少的情况。 可用方案：开源方案 tomcat-redis-session-manager，暂不支持 Tomcat8。 Session 粘滞将用户的每次请求都通过某种方法强制分发到某一个 Web 服务器上，只要这个 Web 服务器上存储了对应 Session 数据，就可以实现会话跟踪。 优点：使用简单，没有额外开销。 缺点：一旦某个 Web 服务器重启或宕机，相对应的 Session 数据将会丢失，而且需要依赖负载均衡机制。 适用场景：对稳定性要求不是很高的业务情景。 Session 集中管理在单独的服务器或服务器集群上使用缓存技术，如 Redis 存储 Session 数据，集中管理所有的 Session，所有的Web服务器都从这个存储介质中存取对应的 Session，实现 Session 共享。 优点：可靠性高，减少 Web 服务器的资源开销。 缺点：实现上有些复杂，配置较多。 适用场景：Web服务器较多、要求高可用性的情况。 可用方案：开源方案 Spring Session，也可以自己实现，主要是重写 HttpServletRequestWrapper 中的 getSession 方法。 基于 Cookie 管理这种方式每次发起请求的时候都需要将 Session 数据放到 Cookie 中传递给服务端。 优点：不需要依赖额外外部存储，不需要额外配置。 缺点：不安全，易被盗取或篡改；Cookie 数量和长度有限制，需要消耗更多网络带宽。 适用场景：数据不重要、不敏感且数据量小的情况。 总结这四种方式，相对来说，Session 集中管理 更加可靠，使用也是最多的。","categories":[],"tags":[]},{"title":"说说反射的用途及实现","slug":"11.Java 基础 - 说说反射的用途及实现","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-28T09:52:45.384Z","comments":true,"path":"2013/07/13/11.Java 基础 - 说说反射的用途及实现/","link":"","permalink":"http://example.com/2013/07/13/11.Java%20%E5%9F%BA%E7%A1%80%20-%20%E8%AF%B4%E8%AF%B4%E5%8F%8D%E5%B0%84%E7%9A%84%E7%94%A8%E9%80%94%E5%8F%8A%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"反射的用途Java 反射机制是一个非常强大的功能，在很多的项目比如 Spring，MyBatis 都都可以看到反射的身影。通过反射机制，我们可以在运行期间获取对象的类型信息。利用这一点我们可以实现工厂模式和代理模式等设计模式，同时也可以解决 Java 泛型擦除等令人苦恼的问题。 反射的实现获取一个对象对应的反射类，在 Java 中有下列方法可以获取一个对象的反射类 通过 getClass() 方法 通过 Class.forName() 方法 使用 类.class 通过类加载器实现，getClassLoader()","categories":[],"tags":[]},{"title":"equals 与 == 的区别","slug":"18.Java 基础 - equals 与 == 的区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T01:46:59.833Z","comments":true,"path":"2013/07/13/18.Java 基础 - equals 与 == 的区别/","link":"","permalink":"http://example.com/2013/07/13/18.Java%20%E5%9F%BA%E7%A1%80%20-%20equals%20%E4%B8%8E%20==%20%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"== 与equals 的主要区别是：== 常用于比较原生类型，而 equals() 方法用于检查对象的相等性。 另一个不同的点是：如果 == 和 equals() 用于比较对象，当两个引用地址相同，== 返回 true。而 equals() 可以返回 true 或者 false 主要取决于重写实现。最常见的一个例子，字符串的比较，不同情况 == 和 equals() 返回不同的结果。","categories":[],"tags":[]},{"title":"MVC 设计思想","slug":"17.Java 基础 - MVC 设计思想","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T01:46:59.833Z","comments":true,"path":"2013/07/13/17.Java 基础 - MVC 设计思想/","link":"","permalink":"http://example.com/2013/07/13/17.Java%20%E5%9F%BA%E7%A1%80%20-%20MVC%20%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3/","excerpt":"","text":"MVC 是三个单词的首字母缩写，它们是 Model（模型）、View（视图）和 Controller（控制）。 这个模式认为，程序不论简单或复杂，从结构上看，都可以分成三层： 最上面的一层，是直接面向最终用户的”视图层”（View）。它是提供给用户的操作界面，是程序的外壳。 最底下的一层，是核心的”数据层”（Model），也就是程序需要操作的数据或信息。 中间的一层，就是”控制层”（Controller），它负责根据用户从”视图层”输入的指令，选取”数据层”中的数据，然后对其进行相应的操作，产生最终结果。","categories":[],"tags":[]},{"title":"List 和 Set 区别","slug":"19.Java 集合 - List 和 Set 区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T01:46:59.833Z","comments":true,"path":"2013/07/13/19.Java 集合 - List 和 Set 区别/","link":"","permalink":"http://example.com/2013/07/13/19.Java%20%E9%9B%86%E5%90%88%20-%20List%20%E5%92%8C%20Set%20%E5%8C%BA%E5%88%AB/","excerpt":"","text":"List, Set 都是继承自 Collection 接口 List 特点：元素有放入顺序，元素可重复。Set 特点：元素无放入顺序，元素不可重复（注意：元素虽然无放入顺序，但是元素在 set 中的位置是有该元素的 HashCode 决定的，其位置其实是固定的） List 接口有三个实现类：LinkedList，ArrayList，Vector。Set 接口有两个实现类：HashSet(底层由 HashMap 实现)，LinkedHashSet","categories":[],"tags":[]},{"title":"ArrayList 与 LinkedList 区别","slug":"21.Java 集合 - ArrayList 与 LinkedList 区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:02:57.827Z","comments":true,"path":"2013/07/13/21.Java 集合 - ArrayList 与 LinkedList 区别/","link":"","permalink":"http://example.com/2013/07/13/21.Java%20%E9%9B%86%E5%90%88%20-%20ArrayList%20%E4%B8%8E%20LinkedList%20%E5%8C%BA%E5%88%AB/","excerpt":"","text":"因为 Array 是基于索引(index)的数据结构，它使用索引在数组中搜索和读取数据是很快的。Array 获取数据的时间复杂度是 O(1),但是要删除数据却是开销很大的，因为这需要重排数组中的所有数据。 相对于 ArrayList，LinkedList 插入是更快的。因为 LinkedList 不像 ArrayList 一样，不需要改变数组的大小，也不需要在数组装满的时候要将所有的数据重新装入一个新的数组，这是 ArrayList 最坏的一种情况，时间复杂度是 O(n)，而 LinkedList 中插入或删除的时间复杂度仅为 O(1)。ArrayList 在插入数据时还需要更新索引（除了插入数组的尾部）。 类似于插入数据，删除数据时，LinkedList 也优于 ArrayList。 LinkedList 需要更多的内存，因为 ArrayList 的每个索引的位置是实际的数据，而 LinkedList 中的每个节点中存储的是实际的数据和前后节点的位置。 你的应用不会随机访问数据。因为如果你需要 LinkedList 中的第 n 个元素的时候，你需要从第一个元素顺序数到第 n 个数据，然后读取数据。 你的应用更多的插入和删除元素，更少的读取数据。因为插入和删除元素不涉及重排数据，所以它要比 ArrayList 要快。","categories":[],"tags":[]},{"title":"ArrayList 与 Vector 区别","slug":"22.Java 集合 - ArrayList 与 Vector 区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:02:57.827Z","comments":true,"path":"2013/07/13/22.Java 集合 - ArrayList 与 Vector 区别/","link":"","permalink":"http://example.com/2013/07/13/22.Java%20%E9%9B%86%E5%90%88%20-%20ArrayList%20%E4%B8%8E%20Vector%20%E5%8C%BA%E5%88%AB/","excerpt":"","text":"同步性：Vector 是线程安全的，也就是说是同步的 ，而 ArrayList 是线程不安全的，不是同步的。 数据增长：当需要增长时，Vector 默认增长为原来一倍 ，而 ArrayList 却是原来的 50% ，这样 ArrayList 就有利于节约内存空间。 说明：如果涉及到堆栈，队列等操作，应该考虑用 Vector，如果需要快速随机访问元素，应该使用 ArrayList","categories":[],"tags":[]},{"title":"HashSet 和 HashMap 区别","slug":"23.Java 集合 - HashSet 和 HashMap 区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:02:57.827Z","comments":true,"path":"2013/07/13/23.Java 集合 - HashSet 和 HashMap 区别/","link":"","permalink":"http://example.com/2013/07/13/23.Java%20%E9%9B%86%E5%90%88%20-%20HashSet%20%E5%92%8C%20HashMap%20%E5%8C%BA%E5%88%AB/","excerpt":"","text":"HashMap HashSet HashMap 实现了 Map 接口 HashSet 实现了 Set 接口 HashMap 储存键值对 HashSet 仅仅存储对象 使用 put() 方法将元素放入 map 中 使用 add() 方法将元素放入 set 中 HashMap 中使用键对象来计算 hashcode 值 HashSet 使用成员对象来计算 hashcode 值，对于两个对象来说 hashcode 可能相同，所以 equals() 方法用来判断对象的相等性，如果两个对象不同的话，那么返回 false HashMap 比较快，因为是使用唯一的键来获取对象 HashSet 较 HashMap 来说比较慢","categories":[],"tags":[]},{"title":"HashMap 和 HashTable 的区别","slug":"24.Java 集合 - HashMap 和 HashTable 的区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:02:57.827Z","comments":true,"path":"2013/07/13/24.Java 集合 - HashMap 和 HashTable 的区别/","link":"","permalink":"http://example.com/2013/07/13/24.Java%20%E9%9B%86%E5%90%88%20-%20HashMap%20%E5%92%8C%20HashTable%20%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"HashMap 几乎可以等价于 HashTable，除了 HashMap 是非 synchronized 的，并可以接受 null(HashMap 可以接受为 null 的键值 (key) 和值 (value)，而 HashTable 则不行)。 HashMap 是非 synchronized，而 HashTable 是 synchronized，这意味着 HashTable 是线程安全的，多个线程可以共享一个 HashTable；而如果没有正确的同步的话，多个线程是不能共享 HashMap 的。Java 5 提供了 ConcurrentHashMap，它是 HashTable 的替代，比 HashTable 的扩展性更好。 另一个区别是 HashMap 的迭代器 (Iterator) 是 fail-fast 迭代器，而 HashTable 的 enumerator 迭代器不是 fail-fast 的。所以当有其它线程改变了 HashMap 的结构（增加或者移除元素），将会抛出 ConcurrentModificationException，但迭代器本身的 remove() 方法移除元素则不会抛出 ConcurrentModificationException 异常。但这并不是一个一定发生的行为，要看 JVM。这条同样也是 Enumeration 和 Iterator 的区别。 由于 HashTable 是线程安全的也是 synchronized，所以在单线程环境下它比 HashMap 要慢。如果你不需要同步，只需要单一线程，那么使用 HashMap 性能要好过 HashTable。 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的。","categories":[],"tags":[]},{"title":"HashMap 和 ConcurrentHashMap 的区别.md","slug":"25.Java 集合 - HashMap 和 ConcurrentHashMap 的区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:02:57.827Z","comments":true,"path":"2013/07/13/25.Java 集合 - HashMap 和 ConcurrentHashMap 的区别/","link":"","permalink":"http://example.com/2013/07/13/25.Java%20%E9%9B%86%E5%90%88%20-%20HashMap%20%E5%92%8C%20ConcurrentHashMap%20%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"放入 HashMap 的元素是 key-value 对。 底层说白了就是散列结构。 要将元素放入到 HashMap 中，那么 key 的类型必须要实现 hashcode 方法，默认这个方法是根据对象的地址来计算的，接着还必须覆盖对象的 equals() 方法。 ConcurrentHashMap 对整个桶数组进行了分段，而 HashMap 则没有 ConcurrentHashMap 在每一个分段上都用锁进行保护，从而让锁的粒度更精细一些，并发性能更好，而 HashMap 没有锁机制，不是线程安全的","categories":[],"tags":[]},{"title":"HashMap 的工作原理及代码实现","slug":"26.Java 集合 - HashMap 的工作原理及代码实现","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:02:57.827Z","comments":true,"path":"2013/07/13/26.Java 集合 - HashMap 的工作原理及代码实现/","link":"","permalink":"http://example.com/2013/07/13/26.Java%20%E9%9B%86%E5%90%88%20-%20HashMap%20%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"HashMap 基于 hashing 原理，我们通过 put() 和 get() 方法储存和获取对象。当我们将键值对传递给 put() 方法时，它调用键对象的 hashCode() 方法来计算 hashcode，让后找到 bucket 位置来储存值对象。当获取对象时，通过键对象的 equals() 方法找到正确的键值对，然后返回值对象。HashMap 使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap 在每个链表节点中储存键值对对象。","categories":[],"tags":[]},{"title":"ConcurrentHashMap 的工作原理及代码实现","slug":"27.Java 集合 - ConcurrentHashMap 的工作原理及代码实现","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:02:57.826Z","comments":true,"path":"2013/07/13/27.Java 集合 - ConcurrentHashMap 的工作原理及代码实现/","link":"","permalink":"http://example.com/2013/07/13/27.Java%20%E9%9B%86%E5%90%88%20-%20ConcurrentHashMap%20%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"ConcurrentHashMap 采用了非常精妙的”分段锁”策略，ConcurrentHashMap 的主干是个 Segment 数组。Segment 继承了 ReentrantLock，所以它就是一种可重入锁（ReentrantLock)。在 ConcurrentHashMap，一个 Segment 就是一个子哈希表，Segment 里维护了一个 HashEntry 数组，并发环境下，对于不同 Segment 的数据进行操作是不用考虑锁竞争的。","categories":[],"tags":[]},{"title":"创建线程的方式及实现","slug":"28.Java 线程 - 创建线程的方式及实现","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:02:57.827Z","comments":true,"path":"2013/07/13/28.Java 线程 - 创建线程的方式及实现/","link":"","permalink":"http://example.com/2013/07/13/28.Java%20%E7%BA%BF%E7%A8%8B%20-%20%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8B%E7%9A%84%E6%96%B9%E5%BC%8F%E5%8F%8A%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"继承 Thread 类创建线程类 定义 Thread 类的子类，并重写该类的 run 方法，该 run 方法的方法体就代表了线程要完成的任务。因此把 run() 方法称为执行体。 创建 Thread 子类的实例，即创建了线程对象。 调用线程对象的 start() 方法来启动该线程。 通过 Runnable 接口创建线程类 定义 Runnable 接口的实现类，并重写该接口的 run() 方法，该 run() 方法的方法体同样是该线程的线程执行体。 创建 Runnable 实现类的实例，并依此实例作为 Thread 的 target 来创建 Thread 对象，该 Thread 对象才是真正的线程对象。 调用线程对象的 start() 方法来启动该线程。 通过 Callable 和 Future 创建线程 创建 Callable 接口的实现类，并实现 call() 方法，该 call() 方法将作为线程执行体，并且有返回值。 创建 Callable 实现类的实例，使用 FutureTask 类来包装 Callable 对象，该 FutureTask 对象封装了该 Callable 对象的 call() 方法的返回值。 使用 FutureTask 对象作为 Thread 对象的 target 创建并启动新线程。 调用 FutureTask 对象的 get() 方法来获得子线程执行结束后的返回值 采用实现 Runnable、Callable 接口的方式创见多线程时： 优势是：线程类只是实现了 Runnable 接口或 Callable 接口，还可以继承其他类。在这种方式下，多个线程可以共享同一个 target 对象，所以非常适合多个相同线程来处理同一份资源的情况，从而可以将 CPU、代码和数据分开，形成清晰的模型，较好地体现了面向对象的思想。 劣势是：编程稍微复杂，如果要访问当前线程，则必须使用 Thread.currentThread() 方法。 使用继承 Thread 类的方式创建多线程时： 优势是：编写简单，如果需要访问当前线程，则无需使用 Thread.currentThread() 方法，直接使用 this 即可获得当前线程。 劣势是：线程类已经继承了 Thread 类，所以不能再继承其他父类。","categories":[],"tags":[]},{"title":"sleep、join、yield 有什么区别","slug":"29.Java 线程 - sleep、join、yield 有什么区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:02:57.826Z","comments":true,"path":"2013/07/13/29.Java 线程 - sleep、join、yield 有什么区别/","link":"","permalink":"http://example.com/2013/07/13/29.Java%20%E7%BA%BF%E7%A8%8B%20-%20sleep%E3%80%81join%E3%80%81yield%20%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/","excerpt":"","text":"sleep()sleep() 方法需要指定等待的时间，它可以让当前正在执行的线程在指定的时间内暂停执行，进入阻塞状态，该方法既可以让其他同优先级或者高优先级的线程得到执行的机会，也可以让低优先级的线程得到执行机会。但是 sleep() 方法不会释放“锁标志”，也就是说如果有 synchronized 同步块，其他线程仍然不能访问共享数据。 wait()wait() 方法需要和 notify() 及 notifyAll() 两个方法一起介绍，这三个方法用于协调多个线程对共享数据的存取，所以必须在 synchronized 语句块内使用，也就是说，调用 wait()，notify() 和 notifyAll() 的任务在调用这些方法前必须拥有对象的锁。注意，它们都是 Object 类的方法，而不是 Thread 类的方法。 wait() 方法与 sleep() 方法的不同之处在于，wait() 方法会释放对象的“锁标志”。当调用某一对象的 wait() 方法后，会使当前线程暂停执行，并将当前线程放入对象等待池中，直到调用了 notify() 方法后，将从对象等待池中移出任意一个线程并放入锁标志等待池中，只有锁标志等待池中的线程可以获取锁标志，它们随时准备争夺锁的拥有权。当调用了某个对象的 notifyAll() 方法，会将对象等待池中的所有线程都移动到该对象的锁标志等待池。 除了使用 notify() 和 notifyAll() 方法，还可以使用带毫秒参数的 wait(long timeout) 方法，效果是在延迟 timeout 毫秒后，被暂停的线程将被恢复到锁标志等待池。 此外，wait()，notify() 及 notifyAll() 只能在 synchronized 语句中使用，但是如果使用的是 ReenTrantLock 实现同步，该如何达到这三个方法的效果呢？解决方法是使用 ReenTrantLock.newCondition() 获取一个 Condition 类对象，然后 Condition 的 await()，signal() 以及 signalAll() 分别对应上面的三个方法。 yield()yield() 方法和 sleep() 方法类似，也不会释放“锁标志”，区别在于，它没有参数，即 yield() 方法只是使当前线程重新回到可执行状态，所以执行 yield() 的线程有可能在进入到可执行状态后马上又被执行，另外 yield() 方法只能使同优先级或者高优先级的线程得到执行机会，这也和 sleep() 方法不同。 join()join() 方法会使当前线程等待调用 join() 方法的线程结束后才能继续执行","categories":[],"tags":[]},{"title":"说说 CountDownLatch 原理","slug":"30.Java 线程 - 说说 CountDownLatch 原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:02:57.826Z","comments":true,"path":"2013/07/13/30.Java 线程 - 说说 CountDownLatch 原理/","link":"","permalink":"http://example.com/2013/07/13/30.Java%20%E7%BA%BF%E7%A8%8B%20-%20%E8%AF%B4%E8%AF%B4%20CountDownLatch%20%E5%8E%9F%E7%90%86/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"说说 CyclicBarrier 原理","slug":"32.Java 线程 - 说说 CyclicBarrier 原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.991Z","comments":true,"path":"2013/07/13/32.Java 线程 - 说说 CyclicBarrier 原理/","link":"","permalink":"http://example.com/2013/07/13/32.Java%20%E7%BA%BF%E7%A8%8B%20-%20%E8%AF%B4%E8%AF%B4%20CyclicBarrier%20%E5%8E%9F%E7%90%86/","excerpt":"","text":"CyclicBarrier 是一个同步辅助类,允许一组线程互相等待,直到到达某个公共屏障点(CommonBarrierPoint)。因为该 barrier 在释放等待线程后可以重用,所以称它为循环的 barrier。","categories":[],"tags":[]},{"title":"说说 CountDownLatch 与 CyclicBarrier 区别","slug":"31.Java 线程 - 说说 CountDownLatch 与 CyclicBarrier 区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.994Z","comments":true,"path":"2013/07/13/31.Java 线程 - 说说 CountDownLatch 与 CyclicBarrier 区别/","link":"","permalink":"http://example.com/2013/07/13/31.Java%20%E7%BA%BF%E7%A8%8B%20-%20%E8%AF%B4%E8%AF%B4%20CountDownLatch%20%E4%B8%8E%20CyclicBarrier%20%E5%8C%BA%E5%88%AB/","excerpt":"","text":"1.CountDownLatch 的作用是允许 1 或 N 个线程等待其他线程完成执行;而 CyclicBarrier 则是允许 N 个线程相互等待。 2.CountDownLatch 的计数器无法被重置; CyclicBarrier 的计数器可以被重置后使用,因此它被称为是循环的 barrier。","categories":[],"tags":[]},{"title":"说说 Semaphore 原理","slug":"33.Java 线程 - 说说 Semaphore 原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.991Z","comments":true,"path":"2013/07/13/33.Java 线程 - 说说 Semaphore 原理/","link":"","permalink":"http://example.com/2013/07/13/33.Java%20%E7%BA%BF%E7%A8%8B%20-%20%E8%AF%B4%E8%AF%B4%20Semaphore%20%E5%8E%9F%E7%90%86/","excerpt":"","text":"Semaphore 直译为信号。实际上 Semaphore 可以看做是一个信号的集合。不同的线程能够从 Semaphore 中获取若干个信号量。当 Semaphore 对象持有的信号量不足时，尝试从 Semaphore 中获取信号的线程将会阻塞。直到其他线程将信号量释放以后，阻塞的线程会被唤醒，重新尝试获取信号量。","categories":[],"tags":[]},{"title":"说说 Exchanger 原理","slug":"34.Java 线程 - 说说 Exchanger 原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.991Z","comments":true,"path":"2013/07/13/34.Java 线程 - 说说 Exchanger 原理/","link":"","permalink":"http://example.com/2013/07/13/34.Java%20%E7%BA%BF%E7%A8%8B%20-%20%E8%AF%B4%E8%AF%B4%20Exchanger%20%E5%8E%9F%E7%90%86/","excerpt":"","text":"当一个线程到达 exchange 调用点时，如果它的伙伴线程此前已经调用了此方法，那么它的伙伴会被调度唤醒并与之进行对象交换，然后各自返回。如果它的伙伴还没到达交换点，那么当前线程将会被挂起，直至伙伴线程到达——完成交换正常返回；或者当前线程被中断——抛出中断异常；又或者是等候超时——抛出超时异常。","categories":[],"tags":[]},{"title":"ThreadLocal 原理分析","slug":"35.Java 线程 - ThreadLocal 原理分析","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.991Z","comments":true,"path":"2013/07/13/35.Java 线程 - ThreadLocal 原理分析/","link":"","permalink":"http://example.com/2013/07/13/35.Java%20%E7%BA%BF%E7%A8%8B%20-%20ThreadLocal%20%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/","excerpt":"","text":"ThreadLocal 提供了线程本地变量，它可以保证访问到的变量属于当前线程，每个线程都保存有一个变量副本，每个线程的变量都不同。ThreadLocal 相当于提供了一种线程隔离，将变量与线程相绑定。","categories":[],"tags":[]},{"title":"讲讲线程池的实现原理","slug":"36.Java 线程 - 讲讲线程池的实现原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.991Z","comments":true,"path":"2013/07/13/36.Java 线程 - 讲讲线程池的实现原理/","link":"","permalink":"http://example.com/2013/07/13/36.Java%20%E7%BA%BF%E7%A8%8B%20-%20%E8%AE%B2%E8%AE%B2%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"当提交一个新任务到线程池时，线程池的处理流程如下： 线程池判断核心线程池里的线程是否都在执行任务。如果不是，则创建一个新的工作线程来执行任务。如果核心线程池里的线程都在执行任务，则进入下个流程。 线程池判断工作队列是否已经满。如果工作队列没有满，则将新提交的任务存储在这个工作队列里。如果工作队列满了，则进入下个流程。 线程池判断线程池的线程是否都处于工作状态。如果没有，则创建一个新的工作线程来执行任务。","categories":[],"tags":[]},{"title":"线程池的几种方式与使用场景","slug":"37.Java 线程 - 线程池的几种方式与使用场景","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.991Z","comments":true,"path":"2013/07/13/37.Java 线程 - 线程池的几种方式与使用场景/","link":"","permalink":"http://example.com/2013/07/13/37.Java%20%E7%BA%BF%E7%A8%8B%20-%20%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%E4%B8%8E%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"","text":"在 Executors 类里面提供了一些静态工厂，生成一些常用的线程池。 newFixedThreadPool：创建固定大小的线程池。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 newCachedThreadPool：创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说 JVM）能够创建的最大线程大小。 newSingleThreadExecutor：创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 newScheduledThreadPool：创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。 newSingleThreadScheduledExecutor：创建一个单线程的线程池。此线程池支持定时以及周期性执行任务的需求。","categories":[],"tags":[]},{"title":"线程的生命周期","slug":"38.Java 线程 - 线程的生命周期","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.991Z","comments":true,"path":"2013/07/13/38.Java 线程 - 线程的生命周期/","link":"","permalink":"http://example.com/2013/07/13/38.Java%20%E7%BA%BF%E7%A8%8B%20-%20%E7%BA%BF%E7%A8%8B%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","excerpt":"","text":"New：新建 Runnable：就绪 Running：运行 Blocked：阻塞 Dead：死亡","categories":[],"tags":[]},{"title":"说说线程安全问题","slug":"39.Java 锁机制 - 说说线程安全问题","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.991Z","comments":true,"path":"2013/07/13/39.Java 锁机制 - 说说线程安全问题/","link":"","permalink":"http://example.com/2013/07/13/39.Java%20%E9%94%81%E6%9C%BA%E5%88%B6%20-%20%E8%AF%B4%E8%AF%B4%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98/","excerpt":"","text":"线程安全是多线程领域的问题，线程安全可以简单理解为一个方法或者一个实例可以在多线程环境中使用而不会出现问题。 在 Java 多线程编程当中，提供了多种实现 Java 线程安全的方式： 最简单的方式，使用 Synchronization 关键字 使用 java.util.concurrent.atomic 包中的原子类，例如 AtomicInteger 使用 java.util.concurrent.locks 包中的锁 使用线程安全的集合 ConcurrentHashMap 使用 volatile 关键字，保证变量可见性（直接从内存读，而不是从线程 cache 读）","categories":[],"tags":[]},{"title":"volatile 实现原理","slug":"40.Java 锁机制 - volatile 实现原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.990Z","comments":true,"path":"2013/07/13/40.Java 锁机制 - volatile 实现原理/","link":"","permalink":"http://example.com/2013/07/13/40.Java%20%E9%94%81%E6%9C%BA%E5%88%B6%20-%20volatile%20%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"在 JVM 底层 volatile 是采用“内存屏障”来实现的 缓存一致性协议（MESI协议）它确保每个缓存中使用的共享变量的副本是一致的。其核心思想如下：当某个 CPU 在写数据时，如果发现操作的变量是共享变量，则会通知其他 CPU 告知该变量的缓存行是无效的，因此其他 CPU 在读取该变量时，发现其无效会重新从主存中加载数据","categories":[],"tags":[]},{"title":"synchronize 实现原理","slug":"41.Java 锁机制 - synchronize 实现原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.990Z","comments":true,"path":"2013/07/13/41.Java 锁机制 - synchronize 实现原理/","link":"","permalink":"http://example.com/2013/07/13/41.Java%20%E9%94%81%E6%9C%BA%E5%88%B6%20-%20synchronize%20%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"同步代码块是使用 monitorenter 和 monitorexit 指令实现的，同步方法（在这看不出来需要看 JVM 底层实现）依靠的是方法修饰符上的 ACC_SYNCHRONIZED 实现。","categories":[],"tags":[]},{"title":"synchronized 与 lock 的区别","slug":"42.Java 锁机制 - synchronized 与 lock 的区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.990Z","comments":true,"path":"2013/07/13/42.Java 锁机制 - synchronized 与 lock 的区别/","link":"","permalink":"http://example.com/2013/07/13/42.Java%20%E9%94%81%E6%9C%BA%E5%88%B6%20-%20synchronized%20%E4%B8%8E%20lock%20%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"synchronized 和 lock 的用法区别 synchronized(隐式锁)：在需要同步的对象中加入此控制，synchronized 可以加在方法上，也可以加在特定代码块中，括号中表示需要锁的对象。 lock（显示锁）：需要显示指定起始位置和终止位置。一般使用 ReentrantLock 类做为锁，多个线程中必须要使用一个 ReentrantLock 类做为对象才能保证锁的生效。且在加锁和解锁处需要通过 lock() 和 unlock() 显示指出。所以一般会在 finally 块中写 unlock() 以防死锁。 synchronized 和 lock 性能区别 synchronized 是托管给 JVM 执行的，而 lock 是 Java 写的控制锁的代码。在 JDK 1.5 中，synchronize 是性能低效的。因为这是一个重量级操作，需要调用操作接口，导致有可能加锁消耗的系统时间比加锁以外的操作还多。相比之下使用 Java 提供的 Lock 对象，性能更高一些。但是到了 JDK 1.6，发生了变化。synchronize 在语义上很清晰，可以进行很多优化，有适应自旋，锁消除，锁粗化，轻量级锁，偏向锁等等。导致在 JDK 1.6 上 synchronize 的性能并不比 Lock 差。 synchronized 和 lock 机制区别 synchronized 原始采用的是 CPU 悲观锁机制，即线程获得的是独占锁。独占锁意味着其 他线程只能依靠阻塞来等待线程释放锁。 Lock 用的是乐观锁方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。乐观锁实现的机制就是 CAS 操作（Compare and Swap）。","categories":[],"tags":[]},{"title":"CAS 乐观锁","slug":"43.Java 锁机制 - CAS 乐观锁","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.990Z","comments":true,"path":"2013/07/13/43.Java 锁机制 - CAS 乐观锁/","link":"","permalink":"http://example.com/2013/07/13/43.Java%20%E9%94%81%E6%9C%BA%E5%88%B6%20-%20CAS%20%E4%B9%90%E8%A7%82%E9%94%81/","excerpt":"","text":"CAS 是项乐观锁技术，当多个线程尝试使用 CAS 同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS 操作包含三个操作数 —— 内存位置（V）、预期原值（A）和新值(B)。如果内存位置的值与预期原值相匹配，那么处理器会自动将该位置值更新为新值。否则，处理器不做任何操作。无论哪种情况，它都会在 CAS 指令之前返回该位置的值。（在 CAS 的一些特殊情况下将仅返回 CAS 是否成功，而不提取当前值。）CAS 有效地说明了“我认为位置 V 应该包含值 A；如果包含该值，则将 B 放到这个位置；否则，不要更改该位置，只告诉我这个位置现在的值即可。”这其实和乐观锁的冲突检查 + 数据更新的原理是一样的。","categories":[],"tags":[]},{"title":"ABA 问题","slug":"44.Java 锁机制 - ABA 问题","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.989Z","comments":true,"path":"2013/07/13/44.Java 锁机制 - ABA 问题/","link":"","permalink":"http://example.com/2013/07/13/44.Java%20%E9%94%81%E6%9C%BA%E5%88%B6%20-%20ABA%20%E9%97%AE%E9%A2%98/","excerpt":"","text":"CAS 会导致“ABA问题”。 CAS 算法实现一个重要前提需要取出内存中某时刻的数据，而在下时刻比较并替换，那么在这个时间差类会导致数据的变化。 比如说一个线程 one 从内存位置 V 中取出 A，这时候另一个线程 two 也从内存中取出 A，并且 two 进行了一些操作变成了 B，然后 two 又将 V 位置的数据变成 A，这时候线程 one 进行 CAS 操作发现内存中仍然是 A，然后 one 操作成功。尽管线程 one 的 CAS 操作成功，但是不代表这个过程就是没有问题的。 部分乐观锁的实现是通过版本号（version）的方式来解决 ABA 问题，乐观锁每次在执行数据的修改操作时，都会带上一个版本号，一旦版本号和数据的版本号一致就可以执行修改操作并对版本号执行 +1 操作，否则就执行失败。因为每次操作的版本号都会随之增加，所以不会出现 ABA 问题，因为版本号只会增加不会减少。","categories":[],"tags":[]},{"title":"乐观锁的业务场景及实现方式","slug":"45.Java 锁机制 - 乐观锁的业务场景及实现方式","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.990Z","comments":true,"path":"2013/07/13/45.Java 锁机制 - 乐观锁的业务场景及实现方式/","link":"","permalink":"http://example.com/2013/07/13/45.Java%20%E9%94%81%E6%9C%BA%E5%88%B6%20-%20%E4%B9%90%E8%A7%82%E9%94%81%E7%9A%84%E4%B8%9A%E5%8A%A1%E5%9C%BA%E6%99%AF%E5%8F%8A%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/","excerpt":"","text":"乐观锁（Optimistic Lock） 每次获取数据的时候，都不会担心数据被修改，所以每次获取数据的时候都不会进行加锁，但是在更新数据的时候需要判断该数据是否被别人修改过。如果数据被其他线程修改，则不进行数据更新，如果数据没有被其他线程修改，则进行数据更新。由于数据没有进行加锁，期间该数据可以被其他线程进行读写操作。 比较适合读取操作比较频繁的场景，如果出现大量的写入操作，数据发生冲突的可能性就会增大，为了保证数据的一致性，应用层需要不断的重新获取数据，这样会增加大量的查询操作，降低了系统的吞吐量。","categories":[],"tags":[]},{"title":"BeanFactory 和 ApplicationContext 有什么区别","slug":"46.Spring - BeanFactory 和 ApplicationContext 有什么区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.988Z","comments":true,"path":"2013/07/13/46.Spring - BeanFactory 和 ApplicationContext 有什么区别/","link":"","permalink":"http://example.com/2013/07/13/46.Spring%20-%20BeanFactory%20%E5%92%8C%20ApplicationContext%20%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/","excerpt":"","text":"BeanFactory 可以理解为含有 bean 集合的工厂类。BeanFactory 包含了种 bean 的定义，以便在接收到客户端请求时将对应的 bean 实例化。 BeanFactory 还能在实例化对象的时生成协作类之间的关系。此举将 bean 自身与 bean 客户端的配置中解放出来。BeanFactory 还包含了 bean 生命周期的控制，调用客户端的初始化方法（initialization methods）和销毁方法（destruction methods）。 从表面上看，ApplicationContext 如同 BeanFactory 一样具有 bean 定义、bean 关联关系的设置，根据请求分发 bean 的功能。但 ApplicationContext 在此基础上还提供了其他的功能： 提供了支持国际化的文本消息 统一的资源文件读取方式 已在监听器中注册的 bean 的事件","categories":[],"tags":[]},{"title":"Spring Bean 的生命周期","slug":"47.Spring - Spring Bean 的生命周期","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.987Z","comments":true,"path":"2013/07/13/47.Spring - Spring Bean 的生命周期/","link":"","permalink":"http://example.com/2013/07/13/47.Spring%20-%20Spring%20Bean%20%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","excerpt":"","text":"Spring Bean 的生命周期简单易懂。在一个 bean 实例被初始化时，需要执行一系列的初始化操作以达到可用的状态。同样的，当一个 bean 不在被调用时需要进行相关的析构操作，并从 bean 容器中移除。 Spring bean factory 负责管理在 spring 容器中被创建的 bean 的生命周期。Bean 的生命周期由两组回调（call back）方法组成。 初始化之后调用的回调方法。 销毁之前调用的回调方法。 Spring 框架提供了以下四种方式来管理 bean 的生命周期事件： InitializingBean 和 DisposableBean 回调接口 针对特殊行为的其他 Aware 接口 Bean 配置文件中的 Custom init() 方法和 destroy() 方法 @PostConstruct 和 @PreDestroy 注解方式","categories":[],"tags":[]},{"title":"Spring IOC 如何实现","slug":"48.Spring - Spring IOC 如何实现","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.987Z","comments":true,"path":"2013/07/13/48.Spring - Spring IOC 如何实现/","link":"","permalink":"http://example.com/2013/07/13/48.Spring%20-%20Spring%20IOC%20%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"Spring 中的 org.springframework.beans 包和 org.springframework.context 包构成了 Spring 框架 IoC 容器的基础。 BeanFactory 接口提供了一个先进的配置机制，使得任何类型的对象的配置成为可能。ApplicationContext 接口对 BeanFactory（是一个子接口）进行了扩展，在 BeanFactory 的基础上添加了其他功能，比如与 Spring 的 AOP 更容易集成，也提供了处理 message resource 的机制（用于国际化）、事件传播以及应用层的特别配置，比如针对 Web 应用的 WebApplicationContext。 org.springframework.beans.factory.BeanFactory 是 Spring IoC 容器的具体实现，用来包装和管理前面提到的各种 bean。BeanFactory 接口是 Spring IoC 容器的核心接口。","categories":[],"tags":[]},{"title":"说说 Spring AOP","slug":"49.Spring - 说说 Spring AOP","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.987Z","comments":true,"path":"2013/07/13/49.Spring - 说说 Spring AOP/","link":"","permalink":"http://example.com/2013/07/13/49.Spring%20-%20%E8%AF%B4%E8%AF%B4%20Spring%20AOP/","excerpt":"","text":"面向切面编程，在我们的应用中，经常需要做一些事情，但是这些事情与核心业务无关，比如，要记录所有 update 方法的执行时间时间，操作人等等信息，记录到日志， 通过 Spring 的 AOP 技术，就可以在不修改 update 的代码的情况下完成该需求。","categories":[],"tags":[]},{"title":"Spring AOP 实现原理","slug":"50.Spring - Spring AOP 实现原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.987Z","comments":true,"path":"2013/07/13/50.Spring - Spring AOP 实现原理/","link":"","permalink":"http://example.com/2013/07/13/50.Spring%20-%20Spring%20AOP%20%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"Spring AOP 中的动态代理主要有两种方式，JDK 动态代理 和 CGLIB 动态代理。JDK 动态代理通过反射来接收被代理的类，并且要求被代理的类必须实现一个接口。JDK 动态代理的核心是 InvocationHandler 接口和 Proxy 类。 如果目标类没有实现接口，那么 Spring AOP 会选择使用 CGLIB 来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成某个类的子类，注意，CGLIB 是通过继承的方式做的动态代理，因此如果某个类被标记为 final，那么它是无法使用 CGLIB 做动态代理的。","categories":[],"tags":[]},{"title":"动态代理（CGLIB 与 JDK）","slug":"51.Spring - 动态代理（CGLIB 与 JDK）","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-29T03:14:41.987Z","comments":true,"path":"2013/07/13/51.Spring - 动态代理（CGLIB 与 JDK）/","link":"","permalink":"http://example.com/2013/07/13/51.Spring%20-%20%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%EF%BC%88CGLIB%20%E4%B8%8E%20JDK%EF%BC%89/","excerpt":"","text":"JDK 动态代理类和委托类需要都实现同一个接口。也就是说只有实现了某个接口的类可以使用 Java 动态代理机制。但是，事实上使用中并不是遇到的所有类都会给你实现一个接口。因此，对于没有实现接口的类，就不能使用该机制。而 CGLIB 则可以实现对类的动态代理。","categories":[],"tags":[]},{"title":"Spring 事务实现方式","slug":"52.Spring - Spring 事务实现方式","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.762Z","comments":true,"path":"2013/07/13/52.Spring - Spring 事务实现方式/","link":"","permalink":"http://example.com/2013/07/13/52.Spring%20-%20Spring%20%E4%BA%8B%E5%8A%A1%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/","excerpt":"","text":"编码方式所谓编程式事务指的是通过编码方式实现事务，即类似于 JDBC 编程实现事务管理。 声明式事务管理方式声明式事务管理又有两种实现方式： 基于 xml 配置文件的方式； 另一个实在业务方法上进行 @Transaction 注解，将事务规则应用到业务逻辑中；","categories":[],"tags":[]},{"title":"Spring 事务底层原理","slug":"53.Spring - Spring 事务底层原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.762Z","comments":true,"path":"2013/07/13/53.Spring - Spring 事务底层原理/","link":"","permalink":"http://example.com/2013/07/13/53.Spring%20-%20Spring%20%E4%BA%8B%E5%8A%A1%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/","excerpt":"","text":"划分处理单元 IOC由于 Spring 解决的问题是对单个数据库进行局部事务处理的，具体的实现首相用 Spring 中的 IOC 划分了事务处理单元。并且将对事务的各种配置放到了 IOC 容器中（设置事务管理器，设置事务的传播特性及隔离机制）。 AOP 拦截需要进行事务处理的类Spring 事务处理模块是通过 AOP 功能来实现声明式事务处理的，具体操作（比如事务实行的配置和读取，事务对象的抽象），用 TransactionProxyFactoryBean 接口来使用 AOP 功能，生成 proxy 代理对象，通过 TransactionInterceptor 完成对代理方法的拦截，将事务处理的功能编织到拦截的方法中。读取 IOC 容器事务配置属性，转化为 Spring 事务处理需要的内部数据结构（TransactionAttributeSourceAdvisor），转化为 TransactionAttribute 表示的数据对象。 对事物处理实现（事务的生成、提交、回滚、挂起）Spring 委托给具体的事务处理器实现。实现了一个抽象和适配。适配的具体事务处理器：DataSource 数据源支持、Hibernate 数据源事务处理支持、JDO 数据源事务处理支持，JPA、JTA 数据源事务处理支持。这些支持都是通过设计 PlatformTransactionManager、AbstractPlatforTransaction 一系列事务处理的支持。 为常用数据源支持提供了一系列的 TransactionManager。 结合PlatformTransactionManager 实现了 TransactionInterception 接口，让其与 TransactionProxyFactoryBean 结合起来，形成一个 Spring 声明式事务处理的设计体系。","categories":[],"tags":[]},{"title":"如何自定义注解实现功能","slug":"54.Spring - 如何自定义注解实现功能","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.762Z","comments":true,"path":"2013/07/13/54.Spring - 如何自定义注解实现功能/","link":"","permalink":"http://example.com/2013/07/13/54.Spring%20-%20%E5%A6%82%E4%BD%95%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%E5%AE%9E%E7%8E%B0%E5%8A%9F%E8%83%BD/","excerpt":"","text":"1.创建自定义注解和创建一个接口相似，但是注解的 interface 关键字需要以 @ 符号开头。 2.注解方法不能带有参数； 3.注解方法返回值类型限定为：基本类型、String、Enums、Annotation 或者是这些类型的数组； 4.注解方法可以有默认值； 5.注解本身能够包含元注解，元注解被用来注解其它注解。","categories":[],"tags":[]},{"title":"Spring MVC 运行流程","slug":"55.Spring - Spring MVC 运行流程","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.762Z","comments":true,"path":"2013/07/13/55.Spring - Spring MVC 运行流程/","link":"","permalink":"http://example.com/2013/07/13/55.Spring%20-%20Spring%20MVC%20%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B/","excerpt":"","text":"Spring MVC 将所有的请求都提交给 DispatcherServlet，它会委托应用系统的其他模块负责对请求进行真正的处理工作。 DispatcherServlet 查询一个或多个 HandlerMapping，找到处理请求的 Controller. DispatcherServlet 请求提交到目标 Controller Controller 进行业务逻辑处理后，会返回一个 ModelAndView Dispatcher 查询一个或多个 ViewResolver 视图解析器,找到 ModelAndView 对象指定的视图对象 视图对象负责渲染返回给客户端。","categories":[],"tags":[]},{"title":"Spring MVC 启动流程","slug":"56.Spring - Spring MVC 启动流程","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.765Z","comments":true,"path":"2013/07/13/56.Spring - Spring MVC 启动流程/","link":"","permalink":"http://example.com/2013/07/13/56.Spring%20-%20Spring%20MVC%20%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/","excerpt":"","text":"在 web.xml 文件中给 Spring MVC 的 Servlet 配置了 load-on-startup，所以程序启动的时候会初始化 Spring MVC，在 HttpServletBean 中将配置的 contextConfigLocation 属性设置到 Servlet 中，然后在 FrameworkServlet 中创建了 WebApplicationContext，DispatcherServlet 根据 contextConfigLocation 配置的 classpath 下的 xml 文件初始化了 Spring MVC 总的组件。","categories":[],"tags":[]},{"title":"Spring 的单例实现原理","slug":"57.Spring - Spring 的单例实现原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.776Z","comments":true,"path":"2013/07/13/57.Spring - Spring 的单例实现原理/","link":"","permalink":"http://example.com/2013/07/13/57.Spring%20-%20Spring%20%E7%9A%84%E5%8D%95%E4%BE%8B%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"Spring 对 Bean 实例的创建是采用单例注册表的方式进行实现的，而这个注册表的缓存是 ConcurrentHashMap 对象。","categories":[],"tags":[]},{"title":"Spring 框架中用到了哪些设计模式","slug":"58.Spring - Spring 框架中用到了哪些设计模式","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.776Z","comments":true,"path":"2013/07/13/58.Spring - Spring 框架中用到了哪些设计模式/","link":"","permalink":"http://example.com/2013/07/13/58.Spring%20-%20Spring%20%E6%A1%86%E6%9E%B6%E4%B8%AD%E7%94%A8%E5%88%B0%E4%BA%86%E5%93%AA%E4%BA%9B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"代理模式：在 AOP 和 Remoting 中被用的比较多。 单例模式：在 Spring 配置文件中定义的 Bean 默认为单例模式。 模板方法：用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate。 前端控制器：Spring 提供了 DispatcherServlet 来对请求进行分发。 视图帮助(View Helper )：Spring 提供了一系列的 JSP 标签，高效宏来辅助将分散的代码整合在视图里。 依赖注入：贯穿于 BeanFactory &#x2F; ApplicationContext 接口的核心理念。 工厂模式：BeanFactory 用来创建对象的实例。","categories":[],"tags":[]},{"title":"谈谈业务中使用分布式的场景","slug":"59.分布式 - 谈谈业务中使用分布式的场景","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:58:55.251Z","comments":true,"path":"2013/07/13/59.分布式 - 谈谈业务中使用分布式的场景/","link":"","permalink":"http://example.com/2013/07/13/59.%E5%88%86%E5%B8%83%E5%BC%8F%20-%20%E8%B0%88%E8%B0%88%E4%B8%9A%E5%8A%A1%E4%B8%AD%E4%BD%BF%E7%94%A8%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E5%9C%BA%E6%99%AF/","excerpt":"","text":"首先，需要了解系统为什么使用分布式。 随着互联网的发展，传统单工程项目的很多性能瓶颈越发凸显，性能瓶颈可以有几个方面： 应用服务层：随着用户量的增加，并发量增加，单项目难以承受如此大的并发请求导致的性能瓶颈。 底层数据库层：随着业务的发展，数据库压力越来越大，导致的性能瓶颈。 场景1：应用系统集群的 Session 共享应用系统集群最简单的就是服务器集群，比如：Tomcat 集群。应用系统集群的时候，比较凸显的问题是 Session 共享，Session 共享我们一是可以通过服务器插件来解决。另外一种也可以通过 Redis 等中间件实现。 场景2：应用系统的服务化拆分服务化拆分，是目前非常火热的一种方式。现在都在提微服务。通过对传统项目进行服务化拆分，达到服务独立解耦，单服务又可以横向扩容。服务化拆分遇到的经典问题就是分布式事务问题。目前，比较常用的分布式事务解决方案有几种：消息最终一致性、TCC 补偿型事务等。 场景3：底层数据库的压力分摊如果系统的性能压力出现在数据库，那我们就可以读写分离、分库分表等方案进行解决。","categories":[],"tags":[]},{"title":"Session 分布式方案","slug":"60.分布式 - Session 分布式方案","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.777Z","comments":true,"path":"2013/07/13/60.分布式 - Session 分布式方案/","link":"","permalink":"http://example.com/2013/07/13/60.%E5%88%86%E5%B8%83%E5%BC%8F%20-%20Session%20%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%B9%E6%A1%88/","excerpt":"","text":"基于 nfs(net filesystem) 的 Session 共享将共享服务器目录 mount 各服务器的本地 session 目录，session 读写受共享服务器 io 限制，不能满足高并发。 基于关系数据库的 Session 共享这种方案普遍使用。使用关系数据库存储 session 数据，对于 mysql 数据库，建议使用 heap 引擎。这种方案性能取决于数据库的性能，在高并发下容易造成表锁（虽然可以采用行锁的存储引擎，性能会下降），并且需要自己实现 session 过期淘汰机制。 基于 Cookie 的 Session 共享这种方案也在大型互联网中普遍使用，将用户的 session 加密序列化后以 cookie 的方式保存在网站根域名下（比如 taobao.com），当用户访问所有二级域名站点式，浏览器会传递所有匹配的根域名的 cookie 信息，这样实现了用户 cookie 化 session 的多服务共享。此方案能够节省大量服务器资源，缺点是存储的信息长度受到 http 协议限制；cookie 的信息还需要做加密解密；请求任何资源时都会将 cookie 附加到 http 头上传到服务器，占用了一定带宽。 基于 Web 容器的 Session 机制利用容器机制，通过配置即可实现。 基于 Zookeeper 的分布式 Session 存储基于 Redis&#x2F;Memcached 的 Session 共享存储这些 key&#x2F;value 非关系存储有较高的性能，轻松达到 2000 左右的 qps，内置的过期机制正好满足 session 的自动实效特性。","categories":[],"tags":[]},{"title":"分布式锁的场景与实现","slug":"61.分布式 - 分布式锁的场景与实现","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.777Z","comments":true,"path":"2013/07/13/61.分布式 - 分布式锁的场景与实现/","link":"","permalink":"http://example.com/2013/07/13/61.%E5%88%86%E5%B8%83%E5%BC%8F%20-%20%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E5%9C%BA%E6%99%AF%E4%B8%8E%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"使用场景首先，我们看这样一个场景：客户下单的时候，我们调用库存中心进行减库存，那我们一般的操作都是： 1update store set num = $num where id = $id 这种通过设置库存的修改方式，我们知道在并发量高的时候会存在数据库的丢失更新，比如 a, b 当前两个事务，查询出来的库存都是 5，a 买了 3 个单子要把库存设置为 2，而 b 买了 1 个单子要把库存设置为 4，那这个时候就会出现 a 会覆盖 b 的更新，所以我们更多的都是会加个条件： 1update store set num = $num where id = $id and num = $query_num 即乐观锁的方式来处理，当然也可以通过版本号来处理乐观锁，都是一样的，但是这是更新一个表，如果我们牵扯到多个表呢，我们希望和这个单子关联的所有的表同一时间只能被一个线程来处理更新，多个线程按照不同的顺序去更新同一个单子关联的不同数据，出现死锁的概率比较大。对于非敏感的数据，我们也没有必要去都加乐观锁处理，我们的服务都是多机器部署的，要保证多进程多线程同时只能有一个进程的一个线程去处理，这个时候我们就需要用到分布式锁。分布式锁的实现方式有很多，我们今天分别通过数据库，Zookeeper, Redis 以及 Tair 的实现逻辑。 数据库实现加 xx 锁更新一个单子关联的所有的数据，先查询出这个单子，并加上排他锁，在进行一系列的更新操作 1234begin transaction；select ...for update；doSomething()；commit(); 这种处理主要依靠排他锁来阻塞其他线程，不过这个需要注意几点： 查询的数据一定要在数据库里存在，如果不存在的话，数据库会加 gap 锁，而 gap 锁之间是兼容的，这种如果两个线程都加了gap 锁，另一个再更新的话会出现死锁。不过一般能更新的数据都是存在的 后续的处理流程需要尽可能的时间短，即在更新的时候提前准备好数据，保证事务处理的时间足够的短，流程足够的短，因为开启事务是一直占着连接的，如果流程比较长会消耗过多的数据库连接的 唯一键通过在一张表里创建唯一键来获取锁，比如执行 saveStore 这个方法 1insert table lock_store (&#x27;method_name&#x27;) values($method_name) 其中 method_name 是个唯一键，通过这种方式也可以做到，解锁的时候直接删除改行记录就行。不过这种方式，锁就不会是阻塞式的，因为插入数据是立马可以得到返回结果的。 那针对以上数据库实现的两种分布式锁，存在什么样的优缺点呢？ 优点简单，方便，快速实现 缺点 基于数据库，开销比较大，性能可能会存在影响 基于数据库的当前读来实现，数据库会在底层做优化，可能用到索引，可能不用到索引，这个依赖于查询计划的分析 Zookeeper 实现获取锁 先有一个锁跟节点，lockRootNode，这可以是一个永久的节点 客户端获取锁，先在 lockRootNode 下创建一个顺序的瞬时节点，保证客户端断开连接，节点也自动删除 调用 lockRootNode 父节点的 getChildren() 方法，获取所有的节点，并从小到大排序，如果创建的最小的节点是当前节点，则返回 true,获取锁成功，否则，关注比自己序号小的节点的释放动作(exist watch)，这样可以保证每一个客户端只需要关注一个节点，不需要关注所有的节点，避免羊群效应。 如果有节点释放操作，重复步骤 3 释放锁只需要删除步骤 2 中创建的节点即可 使用 Zookeeper 的分布式锁存在什么样的优缺点呢？ 优点 客户端如果出现宕机故障的话，锁可以马上释放 可以实现阻塞式锁，通过 watcher 监听，实现起来也比较简单 集群模式，稳定性比较高 缺点 一旦网络有任何的抖动，Zookeeper 就会认为客户端已经宕机，就会断掉连接，其他客户端就可以获取到锁。当然 Zookeeper 有重试机制，这个就比较依赖于其重试机制的策略了 性能上不如缓存 Redis 实现我们先举个例子，比如现在我要更新产品的信息，产品的唯一键就是 productId 简单实现 112345678910111213141516171819202122232425262728293031public boolean lock(String key, V v, int expireTime)&#123; int retry = 0; //获取锁失败最多尝试10次 while (retry &lt; failRetryTimes)&#123; //获取锁 Boolean result = redis.setNx(key, v, expireTime); if (result)&#123; return true; &#125; try &#123; //获取锁失败间隔一段时间重试 TimeUnit.MILLISECONDS.sleep(sleepInterval); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); return false; &#125; &#125; return false; &#125; public boolean unlock(String key)&#123; return redis.delete(key); &#125; public static void main(String[] args) &#123; Integer productId = 324324; RedisLock&lt;Integer&gt; redisLock = new RedisLock&lt;Integer&gt;(); redisLock.lock(productId+&quot;&quot;, productId, 1000); &#125;&#125; 这是一个简单的实现，存在的问题： 可能会导致当前线程的锁误被其他线程释放，比如 a 线程获取到了锁正在执行，但是由于内部流程处理超时或者 gc 导致锁过期，这个时候b线程获取到了锁，a 和 b 线程处理的是同一个 productId，b还在处理的过程中，这个时候 a 处理完了，a 去释放锁，可能就会导致 a 把 b 获取的锁释放了。 不能实现可重入 客户端如果第一次已经设置成功，但是由于超时返回失败，此后客户端尝试会一直失败 针对以上问题我们改进下： v 传 requestId，然后我们在释放锁的时候判断一下，如果是当前 requestId，那就可以释放，否则不允许释放 加入 count 的锁计数，在获取锁的时候查询一次，如果是当前线程已经持有的锁，那锁技术加 1，直接返回 true 简单实现 212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364private static volatile int count = 0;public boolean lock(String key, V v, int expireTime)&#123; int retry = 0; //获取锁失败最多尝试10次 while (retry &lt; failRetryTimes)&#123; //1.先获取锁,如果是当前线程已经持有，则直接返回 //2.防止后面设置锁超时，其实是设置成功，而网络超时导致客户端返回失败，所以获取锁之前需要查询一下 V value = redis.get(key); //如果当前锁存在，并且属于当前线程持有，则锁计数+1，直接返回 if (null != value &amp;&amp; value.equals(v))&#123; count ++; return true; &#125; //如果锁已经被持有了，那需要等待锁的释放 if (value == null || count &lt;= 0)&#123; //获取锁 Boolean result = redis.setNx(key, v, expireTime); if (result)&#123; count = 1; return true; &#125; &#125; try &#123; //获取锁失败间隔一段时间重试 TimeUnit.MILLISECONDS.sleep(sleepInterval); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); return false; &#125; &#125; return false;&#125;public boolean unlock(String key, String requestId)&#123; String value = redis.get(key); if (Strings.isNullOrEmpty(value))&#123; count = 0; return true; &#125; //判断当前锁的持有者是否是当前线程，如果是的话释放锁，不是的话返回false if (value.equals(requestId))&#123; if (count &gt; 1)&#123; count -- ; return true; &#125; boolean delete = redis.delete(key); if (delete)&#123; count = 0; &#125; return delete; &#125; return false;&#125;public static void main(String[] args) &#123; Integer productId = 324324; RedisLock&lt;String&gt; redisLock = new RedisLock&lt;String&gt;(); String requestId = UUID.randomUUID().toString(); redisLock.lock(productId+&quot;&quot;, requestId, 1000);&#125; 这种实现基本解决了误释放和可重入的问题，这里说明几点： 引入 count 实现重入的话，看业务需要，并且在释放锁的时候，其实也可以直接就把锁删除了，一次释放搞定，不需要在通过 count 数量释放多次，看业务需要吧 关于要考虑设置锁超时，所以需要在设置锁的时候查询一次，可能会有性能的考量，看具体业务吧 目前获取锁失败的等待时间是在代码里面设置的，可以提出来，修改下等待的逻辑即可 错误实现获取到锁之后要检查下锁的过期时间，如果锁过期了要重新设置下时间,大致代码如下： 1234567891011121314151617181920public boolean tryLock2(String key, int expireTime)&#123; long expires = System.currentTimeMillis() + expireTime; // 获取锁 Boolean result = redis.setNx(key, expires, expireTime); if (result)&#123; return true; &#125; V value = redis.get(key); if (value != null &amp;&amp; (Long)value &lt; System.currentTimeMillis())&#123; // 锁已经过期 String oldValue = redis.getSet(key, expireTime); if (oldValue != null &amp;&amp; oldValue.equals(value))&#123; return true; &#125; &#125; return false;&#125; 这种实现存在的问题，过度依赖当前服务器的时间了，如果在大量的并发请求下，都判断出了锁过期，而这个时候再去设置锁的时候，最终是会只有一个线程，但是可能会导致不同服务器根据自身不同的时间覆盖掉最终获取锁的那个线程设置的时间。 Tair 实现通过 Tair 来实现分布式锁和 Redis 的实现核心差不多，不过 Tair 有个很方便的 api，感觉是实现分布式锁的最佳配置，就是 Put api 调用的时候需要传入一个 version，就和数据库的乐观锁一样，修改数据之后，版本会自动累加，如果传入的版本和当前数据版本不一致，就不允许修改。","categories":[],"tags":[]},{"title":"分布式事务","slug":"62.分布式 - 分布式事务","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.776Z","comments":true,"path":"2013/07/13/62.分布式 - 分布式事务/","link":"","permalink":"http://example.com/2013/07/13/62.%E5%88%86%E5%B8%83%E5%BC%8F%20-%20%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"分布式一致性在分布式系统中，为了保证数据的高可用，通常，我们会将数据保留多个副本(replica)，这些副本会放置在不同的物理的机器上。为了对用户提供正确的 CRUD 等语义，我们需要保证这些放置在不同物理机器上的副本是一致的。 为了解决这种分布式一致性问题，前人在性能和数据一致性的反反复复权衡过程中总结了许多典型的协议和算法。其中比较著名的有二阶提交协议（Two Phase Commitment Protocol）、三阶提交协议（Three Phase Commitment Protocol） 和 Paxos 算法。 分布式事务 分布式事务是指会涉及到操作多个数据库的事务。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是为了保证分布式系统中的数据一致性。分布式事务处理的关键是必须有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚） 在分布式系统中，各个节点之间在物理上相互独立，通过网络进行沟通和协调。由于存在事务机制，可以保证每个独立节点上的数据操作可以满足 ACID。但是，相互独立的节点之间无法准确的知道其他节点中的事务执行情况。所以从理论上讲，两台机器理论上无法达到一致的状态。如果想让分布式部署的多台机器中的数据保持一致性，那么就要保证在所有节点的数据写操作，要不全部都执行，要么全部的都不执行。但是，一台机器在执行本地事务的时候无法知道其他机器中的本地事务的执行结果。所以他也就不知道本次事务到底应该 commit 还是 rollback。所以，常规的解决办法就是引入一个“协调者”的组件来统一调度所有分布式节点的执行。 XA 规范X&#x2F;Open 组织（即现在的 Open Group ）定义了分布式事务处理模型。 X&#x2F;Open DTP 模型（ 1994 ）包括应用程序（ AP ）、事务管理器（ TM ）、资源管理器（ RM ）、通信资源管理器（ CRM ）四部分。一般，常见的事务管理器（ TM ）是交易中间件，常见的资源管理器（ RM ）是数据库，常见的通信资源管理器（ CRM ）是消息中间件。 通常把一个数据库内部的事务处理，如对多个表的操作，作为本地事务看待。数据库的事务处理对象是本地事务，而分布式事务处理的对象是全局事务。 所谓全局事务，是指分布式事务处理环境中，多个数据库可能需要共同完成一个工作，这个工作即是一个全局事务，例如，一个事务中可能更新几个不同的数据库。对数据库的操作发生在系统的各处但必须全部被提交或回滚。此时一个数据库对自己内部所做操作的提交不仅依赖本身操作是否成功，还要依赖与全局事务相关的其它数据库的操作是否成功，如果任一数据库的任一操作失败，则参与此事务的所有数据库所做的所有操作都必须回滚。 一般情况下，某一数据库无法知道其它数据库在做什么，因此，在一个 DTP 环境中，交易中间件是必需的，由它通知和协调相关数据库的提交或回滚。而一个数据库只将其自己所做的操作（可恢复）影射到全局事务中。 XA 就是 X&#x2F;Open DTP 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供。 二阶提交协议和三阶提交协议就是根据这一思想衍生出来的。可以说二阶段提交其实就是实现 XA 分布式事务的关键(确切地说：两阶段提交主要保证了分布式事务的原子性：即所有结点要么全做要么全不做) 2PC 二阶段提交(Two-phaseCommit)是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。通常，二阶段提交也被称为是一种协议(Protocol))。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，二阶段提交的算法思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。 所谓的两个阶段是指：第一阶段：准备阶段(投票阶段) 和第二阶段：提交阶段（执行阶段）。 准备阶段事务协调者(事务管理器)给每个参与者(资源管理器)发送 Prepare 消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的 redo 和 undo 日志，但不提交，到达一种“万事俱备，只欠东风”的状态。 可以进一步将准备阶段分为以下三个步骤： 协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。 参与者节点执行询问发起为止的所有事务操作，并将 Undo 信息和 Redo 信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作） 各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 提交阶段如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚( Rollback )消息；否则，发送提交( Commit )消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源) 接下来分两种情况分别讨论提交阶段的过程。 当协调者节点从所有参与者节点获得的相应消息都为”同意”时： image 协调者节点向所有参与者节点发出”正式提交( commit )”的请求。 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”完成”消息。 协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。 如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时： image 协调者节点向所有参与者节点发出”回滚操作( rollback )”的请求。 参与者节点利用之前写入的 Undo 信息执行回滚，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”回滚完成”消息。 协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。 不管最后结果如何，第二阶段都会结束当前事务。 二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的： 同步阻塞问题：执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。 单点故障：由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题） 数据不一致：在二阶段提交的阶段二中，当协调者向参与者发送 commit 请求之后，发生了局部网络异常或者在发送 commit 请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了 commit 请求。而在这部分参与者接到 commit 请求之后就会执行 commit 操作。但是其他部分未接到 commit 请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。 二阶段无法解决的问题：协调者再发出 commit 消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。 由于二阶段提交存在着诸如同步阻塞、单点问题、脑裂等缺陷，所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交。 3PC 三阶段提交（Three-phase commit），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。 image 与两阶段提交不同的是，三阶段提交有两个改动点。 引入超时机制。同时在协调者和参与者中都引入超时机制。 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。 也就是说，除了引入超时机制之外，3PC 把 2PC 的准备阶段再次一分为二，这样三阶段提交就有 CanCommit、PreCommit、DoCommit 三个阶段。 CanCommit 阶段3PC 的 CanCommit 阶段其实和 2PC 的准备阶段很像。协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。 事务询问：协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。 响应反馈：参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No PreCommit 阶段协调者根据参与者的反应情况来决定是否可以记性事务的 PreCommit 操作。根据响应情况，有以下两种可能。 1. 假如协调者从所有的参与者获得的反馈都是 Yes 响应，那么就会执行事务的预执行。 发送预提交请求：协调者向参与者发送 PreCommit 请求，并进入 Prepared 阶段。 事务预提交：参与者接收到 PreCommit 请求后，会执行事务操作，并将 undo 和 redo 信息记录到事务日志中。 响应反馈：如果参与者成功的执行了事务操作，则返回 ACK 响应，同时开始等待最终指令。 2. 假如有任何一个参与者向协调者发送了 No 响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。 发送中断请求：协调者向所有参与者发送 abort 请求。 中断事务：参与者收到来自协调者的 abort 请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。 doCommit 阶段该阶段进行真正的事务提交，也可以分为以下两种情况。 1. 执行提交 发送提交请求：协调接收到参与者发送的 ACK 响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送 doCommit 请求。 事务提交：参与者接收到 doCommit 请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。 响应反馈：事务提交完之后，向协调者发送 ACK 响应。 完成事务：协调者接收到所有参与者的 ACK 响应之后，完成事务。 2. 中断事务 协调者没有接收到参与者发送的 ACK 响应（可能是接受者发送的不是 ACK 响应，也可能响应超时），那么就会执行中断事务。 发送中断请求：协调者向所有参与者发送 abort 请求 事务回滚：参与者接收到 abort 请求之后，利用其在阶段二记录的 undo 信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 反馈结果：参与者完成事务回滚之后，向协调者发送 ACK 消息 中断事务：协调者接收到参与者反馈的ACK消息之后，执行事务的中断。 在 doCommit 阶段，如果参与者无法及时接收到来自协调者的 doCommit 或者 abort 请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了 PreCommit 请求，那么协调者产生 PreCommit 请求的前提条件是他在第二阶段开始之前，收到所有参与者的 CanCommit 响应都是 Yes。（一旦参与者收到了 PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到 commit 或者 abort 响应，但是他有理由相信：成功提交的几率很大。） 2PC 与 3PC 的区别相对于 2PC，3PC 主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行 commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的 abort 响应没有及时被参与者接收到，那么参与者在等待超时之后执行了 commit 操作。这样就和其他接到 abort 命令并执行回滚的参与者之间存在数据不一致的情况。","categories":[],"tags":[]},{"title":"前后端分离是如何做的","slug":"64.微服务 - 前后端分离是如何做的","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:33:25.607Z","comments":true,"path":"2013/07/13/64.微服务 - 前后端分离是如何做的/","link":"","permalink":"http://example.com/2013/07/13/64.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E6%98%AF%E5%A6%82%E4%BD%95%E5%81%9A%E7%9A%84/","excerpt":"","text":"在前后端分离架构中，后端只需要负责按照约定的数据格式向前端提供可调用的 API 服务即可。前后端之间通过 HTTP 请求进行交互，前端获取到数据后，进行页面的组装和渲染，最终返回给浏览器。","categories":[],"tags":[]},{"title":"集群与负载均衡的算法与实现","slug":"63.分布式 - 集群与负载均衡的算法与实现","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.776Z","comments":true,"path":"2013/07/13/63.分布式 - 集群与负载均衡的算法与实现/","link":"","permalink":"http://example.com/2013/07/13/63.%E5%88%86%E5%B8%83%E5%BC%8F%20-%20%E9%9B%86%E7%BE%A4%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%9A%84%E7%AE%97%E6%B3%95%E4%B8%8E%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"负载均衡什么是负载均衡呢？用户输入的流量通过负载均衡器按照某种负载均衡算法把流量均匀的分散到后端的多个服务器上，接收到请求的服务器可以独立的响应请求，达到负载分担的目的。从应用场景上来说，常见的负载均衡模型有全局负载均衡和集群内负载均衡，从产品形态角度来说，又可以分为硬件负载均衡和软件负载均衡。全局负载均衡一般通过DNS实现，通过将一个域名解析到不同VIP，来实现不同的region调度能力；硬件负载均衡器常见的有F5、A10、Array，它们的优缺点都比较明显，优点是功能强大，有专门的售后服务团队，性能比较好，缺点是缺少定制的灵活性，维护成本较高；现在的互联网更多的思路是通过软件负载均衡来实现，这样可以满足各种定制化需求，常见的软件负载均衡有 LVS、Nginx、Haproxy。 阿里云高性能负载均衡使用 LVS 和 Tengine，我们在一个 region 区分不同的机房,每个机房都有 LVS 集群和 Tengine 集群，对于用户配置的四层监听，LVS 后面会直接挂载用户 ECS，七层用户监听 ECS 则挂载在 Tengine 上，四层监听的流量直接由 LVS 转发到 ECS，而 7 层监听的流量会经过 LVS 到 Tenigine 再到用户 ECS。每一个 region 里都会有多个可用区，达到主备容灾目的，每一个集群里都有多台设备，第一是为了提升性能，第二也是基于容灾考虑。 图为高性能负载均衡控制管理概要图，SLB 产品也有 SDN 概念，转发和控制是分离的，用户所有配置通过控制台先到控制器，通过集中控制器转换将用户配置推送到不同设备上，每台设备上都有 Agent 接收控制器下发的需求，通过本地转换成 LVS 和 Tengine 能够识别的配置，这个过程支持热配置，不影响用户转发，不需要 reload 才能使新配置生效。 LVSLVS 支持的三种模式 早期 LVS 支持三种模式，DR 模式、TUN 模式和 NAT 模式。 DR 模式DR 模式经过 LVS 之后，LVS 会将 MAC 地址更改、封装 MAC 头，内层 IP 报文不动，报文经过 LVS 负载均衡查找到 RS 之后，将源 MAC 头改成自己的，目的 MAC 改成 RS 地址，MAC 寻址是在二层网络里，对网络部署有一定的限定，在大规模分布式集群部署里，这种模式的灵活性没有办法满足需求； TUN 模式TUN 模式走在 LVS 之后，LVS 会在原有报文基础上封装 IP 头，到了后端 RS 之后，RS 需要解开 IP 报文封装，才能拿到原始报文，不管是 DR 模式还是 TUN 模式，后端 RS 都可以看到真实客户源 IP，目的 IP 是自己的 VIP，VIP 在 RS 设备上需要配置，这样可以直接绕过 LVS 返回给用户，TUN 模式问题在于需要在后端 ECS 上配置解封装模块，在 Linux 上已经支持这种模块，但是 Windows 上还没有提供支持，所以会对用户系统镜像选择有限定。 NAT 模式NAT 模式用户访问的是 VIP，LVS 查找完后会将目的 IP 做 DNAT 转换，选择出 RS 地址，因为客户端的 IP 没变，在回包的时候直接向公网真实客户端 IP 去路由，NAT 的约束是因为 LVS 做了 DNAT 转换，所以回包需要走 LVS，把报文头转换回去，由于 ECS 看到的是客户端真实的源地址，我们需要在用户 ECS 上配置路由，将到 ECS 的默认路由指向 LVS 上，这对用户场景也做了限制。 LVS 基于 NetFilter 框架实现 NetFilter 是 Linux 提供的网络开放平台，基于平台可以开发自己的业务功能模块，早期好多安全厂商都是基于 NetFilter 做一些业务模型实现，这种模型比较灵活，但通用模型里更多的是兼容性考虑，路径会非常长；而且通用模型中没办法发挥多核特性，目前 CPU 的发展更多是向横向扩展，我们经常见到多路服务器，每路上有多少核，早期通用模型对多核支持并不是特别友善，在多核设计上有些欠缺，导致我们在通用模型上做一些应用开发时的扩展性是有限的，随着核的数量越来越多，性能不增反降。 LVS 的改进 早期模式的各种限制制约了我们的发展，所以我们首先做了 FullNAT，相比原来的 NAT 方式，FullNAT 多了 SNAT 属性，将客户端的原 IP 地址作了转换；其次，我们在并行化上做了处理，充分利用多核实现性能线性提升；然后是快速路径，我们在做网络转发模型时很容易想到设计快速路径和慢速路径，慢速路径更多是解决首包如何通过设备问题，可能需要查ACL或路由，需要判断许多和策略相关的东西，后面所有报文都可以通过快速路径转发出去；还有指令相关优化，利用因特尔特殊指令提升性能；另外针对多核架构，NUMA 多节点内存访问，通过访问 Local 节点内存可能获得更好的延迟表现。 image 客户端进来 IP 首先访问 LVS 的 VIP，原 IP 是客户端的，目的 IP 是 LVS 的 VIP，经过 FullNAT 转换后，原 IP 变成 LVS 的 Local 地址，目的地址是 LVS 选择出来的 RS 地址，这样在 RS 回包时比较容易，只要路由可达，报文一定会交到 LVS 上，不需要在 RS 上做特殊的配置。右面就是 DNAT + SNAT 转换，报文就可以通过 LVS 转发回客户端，这种方式主要带来应用场景部署灵活性选择。 通过并行化实现对 LVS 性能的改善，性能没有办法得到线性提升更多的是因为每条路径都需要访问全局资源，就会不可避免引入锁的开箱，另外，同一条链接上的报文可能分散在不同的核上，大家去访问全局资源时也会导致 cache 的丢失。所以我们通过 RSS 技术把同一个五源组报文扔到同一个 CPU 上处理，保证入方向的所有相同连接上的报文都能交给相同 CPU 处理，每个核在转发出去时都用当前 CPU 上的 Local 地址，通过设置一些 fdir 规则，报文回来时后端 RS 访问的目的地址就是对应 CPU 上的 local 地址，可以交到指定的 CPU 上去处理，这样一条连接上左右方向报文都可以交给同一个 CPU 处理，将流在不同的 CPU 隔离开；另外，我们把所有配置资源包括动态缓存资源在每个 CPU 上作了拷贝，将资源局部化，这使整个流从进入 LVS 到转发出去访问的资源都是固定在一个核上的本地资源，使性能达到最大化，实现线性提升。 改进后的 LVS 表现如下： 出于对容灾和性能提升的考虑，我们做了集群化部署，每个 region 有不同机房，每个机房有多个调度单元，每个单元有多台 LVS 设备； 每台 LVS 经过优化后，都能达到更高性能，大容量，单台 LVS 可以达到 4000W PPS，600W CPS、单个 group 可以到达 1亿 并发； 支持 region、IDC、集群和应用级的高可用； 实现了防攻击功能，并在原版 LVS 上提供了更丰富的功能，可以基于各个维度做管理控制，精确的统计，流量的分析等。 Tengine Tengine 在应用过程中也遇到了各种问题，最严重的就是性能问题，我们发现随着 CPU 数量越来越多，QPS 值并没有线性提升；Nginx 本身是多 worker 模型，每个 worker 是单进程模式，多 worker 架构做 CPU 亲和，内部基于事件驱动的模型，其本身已经提供了很高的性能，单核 Nginx 可以跑到 1W5～2W QPS。Nginx 往下第一层是 socket API，socket 往下有一层 VFS，再往下是 TCP、IP，socket 层比较薄，经过量化的分析和评估，性能开销最大的是 TCP 协议栈和 VFS 部分，因为同步开销大，我们发现横向扩展不行，对此，我们做了一些优化。 七层反向代理的路径更长，处理更复杂，所以它的性能比 LVS 低很多，我们比较关注单机和集群的性能，集群性能可以靠堆设备去解决，单机如果不提升，成本会一直增加，从性能角度来看，有以下的优化思路和方向： 基于 Kernel 做开发，比如优化协议栈； 基于 AliSocket 的优化，AliSocket 是阿里研发的高性能 TCP 协议栈平台，底层是 DPDK，它将资源做了局部化处理，报文分发不同核处理，性能非常出色； HTTPS 业务越来越多，流量逐步递增，我们采用硬件加速卡方式做一些加解密的性能提升，还有 HTTPS 的会话复用； 基于 Web 传输层的性能优化","categories":[],"tags":[]},{"title":"如何解决跨域","slug":"65.微服务 - 如何解决跨域","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.777Z","comments":true,"path":"2013/07/13/65.微服务 - 如何解决跨域/","link":"","permalink":"http://example.com/2013/07/13/65.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E8%B7%A8%E5%9F%9F/","excerpt":"","text":"什么是跨域问题？跨域，指的是浏览器不能执行其他网站的脚本。它是由浏览器的同源策略造成的，是浏览器对 JavaScript 施加的安全限制。 什么是同源？所谓同源是指，域名，协议，端口均相同 http://www.funtl.com --&gt; http://admin.funtl.com 跨域 http://www.funtl.com --&gt; http://www.funtl.com 非跨域 http://www.funtl.com --&gt; http://www.funtl.com:8080 跨域 http://www.funtl.com --&gt; https://www.funtl.com 跨域 使用 CORS（跨资源共享）解决跨域问题CORS 是一个 W3C 标准，全称是”跨域资源共享”（Cross-origin resource sharing）。它允许浏览器向跨源服务器，发出 XMLHttpRequest 请求，从而克服了 AJAX 只能同源使用的限制。 CORS 需要浏览器和服务器同时支持。目前，所有浏览器都支持该功能，IE 浏览器不能低于 IE10 整个 CORS 通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS 通信与同源的 AJAX 通信没有差别，代码完全一样。浏览器一旦发现 AJAX 请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，但用户不会有感觉 因此，实现 CORS 通信的关键是服务器。只要服务器实现了 CORS 接口，就可以跨源通信 CORS 与 JSONP 的比较CORS 与 JSONP 的使用目的相同，但是比 JSONP 更强大。 JSONP 只支持 GET 请求，CORS 支持所有类型的 HTTP 请求。JSONP 的优势在于支持老式浏览器，以及可以向不支持 CORS 的网站请求数据。","categories":[],"tags":[]},{"title":"微服务哪些框架","slug":"66.微服务 - 微服务哪些框架","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.777Z","comments":true,"path":"2013/07/13/66.微服务 - 微服务哪些框架/","link":"","permalink":"http://example.com/2013/07/13/66.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%93%AA%E4%BA%9B%E6%A1%86%E6%9E%B6/","excerpt":"","text":"Dubbo是阿里巴巴服务化治理的核心框架，并被广泛应用于阿里巴巴集团的各成员站点。阿里巴巴近几年对开源社区的贡献不论在国内还是国外都是引人注目的，比如：JStorm 捐赠给 Apache 并加入 Apache 基金会等，为中国互联网人争足了面子，使得阿里巴巴在国人眼里已经从电商升级为一家科技公司了。 Spring Cloud从命名我们就可以知道，它是 Spring Source 的产物，Spring 社区的强大背书可以说是 Java 企业界最有影响力的组织了，除了 Spring Source 之外，还有 Pivotal 和 Netflix 是其强大的后盾与技术输出。其中 Netflix 开源的整套微服务架构套件是 Spring Cloud 的核心。","categories":[],"tags":[]},{"title":"你怎么理解 RPC 框架","slug":"67.微服务 - 你怎么理解 RPC 框架","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.777Z","comments":true,"path":"2013/07/13/67.微服务 - 你怎么理解 RPC 框架/","link":"","permalink":"http://example.com/2013/07/13/67.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E4%BD%A0%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%20RPC%20%E6%A1%86%E6%9E%B6/","excerpt":"","text":"什么是 RPC？RPC 是指远程过程调用，也就是说两台服务器 A，B 一个应用部署在 A 服务器上，想要调用 B 服务器上应用提供的函数或方法，由于不在一个内存空间，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。 RPC 是如何通讯的？ 要解决通讯的问题，主要是通过在客户端和服务器之间建立 TCP 连接，远程过程调用的所有交换的数据都在这个连接里传输。连接可以是按需连接，调用结束后就断掉，也可以是长连接，多个远程过程调用共享同一个连接。 要解决寻址的问题，也就是说，A 服务器上的应用怎么告诉底层的 RPC 框架，如何连接到 B 服务器（如主机或 IP 地址）以及特定的端口，方法的名称是什么，这样才能完成调用。比如基于 Web 服务协议栈的 RPC，就要提供一个 endpoint URI，或者是从 UDDI 服务上查找。如果是 RMI 调用的话，还需要一个 RMI Registry 来注册服务的地址。 当 A 服务器上的应用发起远程过程调用时，方法的参数需要通过底层的网络协议如 TCP 传递到 B 服务器，由于网络协议是基于二进制的，内存中的参数的值要序列化成二进制的形式，也就是序列化（Serialize）或编组（marshal），通过寻址和传输将序列化的二进制发送给 B 服务器。 B 服务器收到请求后，需要对参数进行反序列化（序列化的逆操作），恢复为内存中的表达方式，然后找到对应的方法（寻址的一部分）进行本地调用，然后得到返回值。 返回值还要发送回服务器 A 上的应用，也要经过序列化的方式发送，服务器 A 接到后，再反序列化，恢复为内存中的表达方式，交给 A 服务器上的应用。 为什么要用 RPC？就是无法在一个进程内，甚至一个计算机内通过本地调用的方式完成的需求，比如比如不同的系统间的通讯，甚至不同的组织间的通讯。由于计算能力需要横向扩展，需要在多台机器组成的集群上部署应用，","categories":[],"tags":[]},{"title":"说说 RPC 的实现原理","slug":"68.微服务 - 说说 RPC 的实现原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:32:16.796Z","comments":true,"path":"2013/07/13/68.微服务 - 说说 RPC 的实现原理/","link":"","permalink":"http://example.com/2013/07/13/68.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E8%AF%B4%E8%AF%B4%20RPC%20%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"首先需要有处理网络连接通讯的模块，负责连接建立、管理和消息的传输。其次需要有编解码的模块，因为网络通讯都是传输的字节码，需要将我们使用的对象序列化和反序列化。剩下的就是客户端和服务器端的部分，服务器端暴露要开放的服务接口，客户调用服务接口的一个代理实现，这个代理实现负责收集数据、编码并传输给服务器然后等待结果返回。","categories":[],"tags":[]},{"title":"说说 Dubbo 的实现原理","slug":"69.微服务 - 说说 Dubbo 的实现原理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.777Z","comments":true,"path":"2013/07/13/69.微服务 - 说说 Dubbo 的实现原理/","link":"","permalink":"http://example.com/2013/07/13/69.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E8%AF%B4%E8%AF%B4%20Dubbo%20%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"Dubbo 作为 RPC 框架，实现的效果就是调用远程的方法就像在本地调用一样。如何做到呢？ 本地有对远程方法的描述，包括方法名、参数、返回值，在 Dubbo 中是远程和本地使用同样的接口 要有对网络通信的封装，要对调用方来说通信细节是完全不可见的，网络通信要做的就是将调用方法的属性通过一定的协议（简单来说就是消息格式）传递到服务端 服务端按照协议解析出调用的信息；执行相应的方法；在将方法的返回值通过协议传递给客户端；客户端再解析；在调用方式上又可以分为同步调用和异步调用；","categories":[],"tags":[]},{"title":"你怎么理解 RESTful","slug":"70.微服务 - 你怎么理解 RESTful","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:30:59.538Z","comments":true,"path":"2013/07/13/70.微服务 - 你怎么理解 RESTful/","link":"","permalink":"http://example.com/2013/07/13/70.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E4%BD%A0%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%20RESTful/","excerpt":"","text":"2000 年，Roy Thomas Fielding 博士在他那篇著名的博士论文《Architectural Styles and the Design of Network-based Software Architectures》中提出了几种软件应用的架构风格，REST 作为其中的一种架构风格在这篇论文的第5章中进行了概括性的介绍。 REST 是“REpresentational State Transfer”的缩写，可以翻译成“表现状态转换”，但是在绝大多数场合中我们只说 REST 或者 RESTful。Fielding 在论文中将 REST 定位为“分布式超媒体应用（Distributed Hypermedia System）”的架构风格，它在文中提到一个名为“HATEOAS（Hypermedia as the engine of application state）”的概念。 我们利用一个面向最终用户的 Web 应用来对这个概念进行简单阐述：这里所谓的应用状态（Application State）表示 Web 应用的客户端的状态，简单起见可以理解为会话状态。资源在浏览器中以超媒体的形式呈现，通过点击超媒体中的链接可以获取其它相关的资源或者对当前资源进行相应的处理，获取的资源或者针对资源处理的响应同样以超媒体的形式再次呈现在浏览器上。由此可见，超媒体成为了驱动客户端会话状态的转换的引擎。 借助于超媒体这种特殊的资源呈现方式，应用状态的转换体现为浏览器中呈现资源的转换。如果将超媒体进一步抽象成一般意义上的资源呈现（Representation ）方式，那么应用状态变成了可被呈现的状态（REpresentational State）。应用状态之间的转换就成了可被呈现的状态装换（REpresentational State Transfer），这就是 REST。 总结REST 是一种很笼统的概念，它代表一种架构风格。","categories":[],"tags":[]},{"title":"说说如何设计一个良好的 API","slug":"71.微服务 - 说说如何设计一个良好的 API","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T07:09:14.777Z","comments":true,"path":"2013/07/13/71.微服务 - 说说如何设计一个良好的 API/","link":"","permalink":"http://example.com/2013/07/13/71.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E8%AF%B4%E8%AF%B4%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E4%B8%80%E4%B8%AA%E8%89%AF%E5%A5%BD%E7%9A%84%20API/","excerpt":"","text":"版本号在 RESTful API 中，API 接口应该尽量兼容之前的版本。但是，在实际业务开发场景中，可能随着业务需求的不断迭代，现有的 API 接口无法支持旧版本的适配，此时如果强制升级服务端的 API 接口将导致客户端旧有功能出现故障。实际上，Web 端是部署在服务器，因此它可以很容易为了适配服务端的新的 API 接口进行版本升级，然而像 Android 端、IOS 端、PC 端等其他客户端是运行在用户的机器上，因此当前产品很难做到适配新的服务端的 API 接口，从而出现功能故障，这种情况下，用户必须升级产品到最新的版本才能正常使用。 为了解决这个版本不兼容问题，在设计 RESTful API 的一种实用的做法是使用版本号。一般情况下，我们会在 url 中保留版本号，并同时兼容多个版本。 12【GET】 /v1/users/&#123;user_id&#125; // 版本 v1 的查询用户列表的 API 接口【GET】 /v2/users/&#123;user_id&#125; // 版本 v2 的查询用户列表的 API 接口 现在，我们可以不改变版本 v1 的查询用户列表的 API 接口的情况下，新增版本 v2 的查询用户列表的 API 接口以满足新的业务需求，此时，客户端的产品的新功能将请求新的服务端的 API 接口地址。虽然服务端会同时兼容多个版本，但是同时维护太多版本对于服务端而言是个不小的负担，因为服务端要维护多套代码。这种情况下，常见的做法不是维护所有的兼容版本，而是只维护最新的几个兼容版本，例如维护最新的三个兼容版本。在一段时间后，当绝大多数用户升级到较新的版本后，废弃一些使用量较少的服务端的老版本API 接口版本，并要求使用产品的非常旧的版本的用户强制升级。 注意的是，“不改变版本 v1 的查询用户列表的 API 接口”主要指的是对于客户端的调用者而言它看起来是没有改变。而实际上，如果业务变化太大，服务端的开发人员需要对旧版本的 API 接口使用适配器模式将请求适配到新的API 接口上。 资源路径RESTful API 的设计以资源为核心，每一个 URI 代表一种资源。因此，URI 不能包含动词，只能是名词。注意的是，形容词也是可以使用的，但是尽量少用。一般来说，不论资源是单个还是多个，API 的名词要以复数进行命名。此外，命名名词的时候，要使用小写、数字及下划线来区分多个单词。这样的设计是为了与 json 对象及属性的命名方案保持一致。例如，一个查询系统标签的接口可以进行如下设计。 1【GET】 /v1/tags/&#123;tag_id&#125; 同时，资源的路径应该从根到子依次如下 1/&#123;resources&#125;/&#123;resource_id&#125;/&#123;sub_resources&#125;/&#123;sub_resource_id&#125;/&#123;sub_resource_property&#125; 我们来看一个“添加用户的角色”的设计，其中“用户”是主资源，“角色”是子资源。 1【POST】 /v1/users/&#123;user_id&#125;/roles/&#123;role_id&#125; // 添加用户的角色 有的时候，当一个资源变化难以使用标准的 RESTful API 来命名，可以考虑使用一些特殊的 actions 命名。 1/&#123;resources&#125;/&#123;resource_id&#125;/actions/&#123;action&#125; 举个例子，“密码修改”这个接口的命名很难完全使用名词来构建路径，此时可以引入 action 命名。 1【PUT】 /v1/users/&#123;user_id&#125;/password/actions/modify // 密码修改 请求方式可以通过 GET、 POST、 PUT、 PATCH、 DELETE 等方式对服务端的资源进行操作。其中： GET：用于查询资源 POST：用于创建资源 PUT：用于更新服务端的资源的全部信息 PATCH：用于更新服务端的资源的部分信息 DELETE：用于删除服务端的资源。 这里，使用“用户”的案例进行回顾通过 GET、 POST、 PUT、 PATCH、 DELETE 等方式对服务端的资源进行操作。 123456【GET】 /users # 查询用户信息列表【GET】 /users/1001 # 查看某个用户信息【POST】 /users # 新建用户信息【PUT】 /users/1001 # 更新用户信息(全部字段)【PATCH】 /users/1001 # 更新用户信息(部分字段)【DELETE】 /users/1001 # 删除用户信息 查询参数RESTful API 接口应该提供参数，过滤返回结果。其中，offset 指定返回记录的开始位置。一般情况下，它会结合 limit 来做分页的查询，这里 limit 指定返回记录的数量。 1【GET】 /&#123;version&#125;/&#123;resources&#125;/&#123;resource_id&#125;?offset=0&amp;limit=20 同时，orderby 可以用来排序，但仅支持单个字符的排序，如果存在多个字段排序，需要业务中扩展其他参数进行支持。 1【GET】 /&#123;version&#125;/&#123;resources&#125;/&#123;resource_id&#125;?orderby=&#123;field&#125; [asc|desc] 为了更好地选择是否支持查询总数，我们可以使用 count 字段，count 表示返回数据是否包含总条数，它的默认值为 false。 1【GET】 /&#123;version&#125;/&#123;resources&#125;/&#123;resource_id&#125;?count=[true|false] 上面介绍的 offset、 limit、 orderby 是一些公共参数。此外，业务场景中还存在许多个性化的参数。我们来看一个例子。 1【GET】 /v1/categorys/&#123;category_id&#125;/apps/&#123;app_id&#125;?enable=[1|0]&amp;os_type=&#123;field&#125;&amp;device_ids=&#123;field,field,…&#125; 注意的是，不要过度设计，只返回用户需要的查询参数。此外，需要考虑是否对查询参数创建数据库索引以提高查询性能。 状态码使用适合的状态码很重要，而不应该全部都返回状态码 200，或者随便乱使用。这里，列举在实际开发过程中常用的一些状态码，以供参考。 状态码 描述 200 请求成功 201 创建成功 400 错误的请求 401 未验证 403 被拒绝 404 无法找到 409 资源冲突 500 服务器内部错误 异常响应当 RESTful API 接口出现非 2xx 的 HTTP 错误码响应时，采用全局的异常结构响应信息。 12345678910HTTP/1.1 400 Bad RequestContent-Type: application/json&#123; &quot;code&quot;: &quot;INVALID_ARGUMENT&quot;, &quot;message&quot;: &quot;&#123;error message&#125;&quot;, &quot;cause&quot;: &quot;&#123;cause message&#125;&quot;, &quot;request_id&quot;: &quot;01234567-89ab-cdef-0123-456789abcdef&quot;, &quot;host_id&quot;: &quot;&#123;server identity&#125;&quot;, &quot;server_time&quot;: &quot;2014-01-01T12:00:00Z&quot;&#125; 请求参数在设计服务端的 RESTful API 的时候，我们还需要对请求参数进行限制说明。例如一个支持批量查询的接口，我们要考虑最大支持查询的数量。 123【GET】 /v1/users/batch?user_ids=1001,1002 // 批量查询用户信息参数说明- user_ids: 用户ID串，最多允许 20 个。 此外，在设计新增或修改接口时，我们还需要在文档中明确告诉调用者哪些参数是必填项，哪些是选填项，以及它们的边界值的限制。 12345678910【POST】 /v1/users // 创建用户信息请求内容&#123; &quot;username&quot;: &quot;lusifer&quot;, // 必填, 用户名称, max 10 &quot;realname&quot;: &quot;鲁斯菲尔&quot;, // 必填, 用户名称, max 10 &quot;password&quot;: &quot;123456&quot;, // 必填, 用户密码, max 32 &quot;email&quot;: &quot;topsale@vip.qq.com&quot;, // 选填, 电子邮箱, max 32 &quot;weixin&quot;: &quot;Lusifer&quot;, // 选填，微信账号, max 32 &quot;sex&quot;: 1 // 必填, 用户性别[1-男 2-女 99-未知]&#125; 响应参数针对不同操作，服务端向用户返回的结果应该符合以下规范。 1234567【GET】 /&#123;version&#125;/&#123;resources&#125;/&#123;resource_id&#125; // 返回单个资源对象【GET】 /&#123;version&#125;/&#123;resources&#125; // 返回资源对象的列表【POST】 /&#123;version&#125;/&#123;resources&#125; // 返回新生成的资源对象【PUT】 /&#123;version&#125;/&#123;resources&#125;/&#123;resource_id&#125; // 返回完整的资源对象【PATCH】 /&#123;version&#125;/&#123;resources&#125;/&#123;resource_id&#125; // 返回完整的资源对象【DELETE】 /&#123;version&#125;/&#123;resources&#125;/&#123;resource_id&#125; // 状态码 200，返回完整的资源对象。 // 状态码 204，返回一个空文档 如果是单条数据，则返回一个对象的 JSON 字符串。 12345678HTTP/1.1 200 OK&#123; &quot;id&quot; : &quot;01234567-89ab-cdef-0123-456789abcdef&quot;, &quot;name&quot; : &quot;example&quot;, &quot;created_time&quot;: 1496676420000, &quot;updated_time&quot;: 1496676420000, ...&#125; 如果是列表数据，则返回一个封装的结构体。 1234567891011121314HTTP/1.1 200 OK&#123; &quot;count&quot;:100, &quot;items&quot;:[ &#123; &quot;id&quot; : &quot;01234567-89ab-cdef-0123-456789abcdef&quot;, &quot;name&quot; : &quot;example&quot;, &quot;created_time&quot;: 1496676420000, &quot;updated_time&quot;: 1496676420000, ... &#125;, ... ]&#125; 一个完整的案例最后，我们使用一个完整的案例将前面介绍的知识整合起来。这里，使用“获取用户列表”的案例。 123456789101112131415161718192021222324252627282930313233343536【GET】 /v1/users?[&amp;keyword=xxx][&amp;enable=1][&amp;offset=0][&amp;limit=20] 获取用户列表功能说明：获取用户列表请求方式：GET参数说明- keyword: 模糊查找的关键字。[选填]- enable: 启用状态[1-启用 2-禁用]。[选填]- offset: 获取位置偏移，从 0 开始。[选填]- limit: 每次获取返回的条数，缺省为 20 条，最大不超过 100。 [选填]响应内容HTTP/1.1 200 OK&#123; &quot;count&quot;:100, &quot;items&quot;:[ &#123; &quot;id&quot; : &quot;01234567-89ab-cdef-0123-456789abcdef&quot;, &quot;name&quot; : &quot;example&quot;, &quot;created_time&quot;: 1496676420000, &quot;updated_time&quot;: 1496676420000, ... &#125;, ... ]&#125;失败响应HTTP/1.1 403 UC/AUTH_DENIEDContent-Type: application/json&#123; &quot;code&quot;: &quot;INVALID_ARGUMENT&quot;, &quot;message&quot;: &quot;&#123;error message&#125;&quot;, &quot;cause&quot;: &quot;&#123;cause message&#125;&quot;, &quot;request_id&quot;: &quot;01234567-89ab-cdef-0123-456789abcdef&quot;, &quot;host_id&quot;: &quot;&#123;server identity&#125;&quot;, &quot;server_time&quot;: &quot;2014-01-01T12:00:00Z&quot;&#125;错误代码- 403 UC/AUTH_DENIED 授权受限","categories":[],"tags":[]},{"title":"如何理解 RESTful API 的幂等性","slug":"72.微服务 - 如何理解 RESTful API 的幂等性","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:30:19.888Z","comments":true,"path":"2013/07/13/72.微服务 - 如何理解 RESTful API 的幂等性/","link":"","permalink":"http://example.com/2013/07/13/72.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%20RESTful%20API%20%E7%9A%84%E5%B9%82%E7%AD%89%E6%80%A7/","excerpt":"","text":"什么是幂等性HTTP 幂等方法，是指无论调用多少次都不会有不同结果的 HTTP 方法。不管你调用一次，还是调用一百次，一千次，结果都是相同的。 123456GET /tickets # 获取ticket列表GET /tickets/12 # 查看某个具体的ticketPOST /tickets # 新建一个ticketPUT /tickets/12 # 更新ticket 12PATCH /tickets/12 # 更新ticket 12DELETE /tickets/12 # 删除ticekt 12 HTTP GET 方法HTTP GET 方法，用于获取资源，不管调用多少次接口，结果都不会改变，所以是幂等的。 12GET /tickets # 获取ticket列表GET /tickets/12 # 查看某个具体的ticket 只是查询数据，不会影响到资源的变化，因此我们认为它幂等。 值得注意，幂等性指的是作用于结果而非资源本身。怎么理解呢？例如，这个 HTTP GET 方法可能会每次得到不同的返回内容，但并不影响资源。 可能你会问有这种情况么？当然有咯。例如，我们有一个接口获取当前时间，我们就应该设计成 1GET /service_time # 获取服务器当前时间 它本身不会对资源本身产生影响，因此满足幂等性。 HTTP POST 方法HTTP POST 方法是一个非幂等方法，因为调用多次，都将产生新的资源。 1POST /tickets # 新建一个ticket 因为它会对资源本身产生影响，每次调用都会有新的资源产生，因此不满足幂等性。 HTTP PUT 方法HTTP PUT 方法是不是幂等的呢？我们来看下 1PUT /tickets/12 # 更新ticket 12 因为它直接把实体部分的数据替换到服务器的资源，我们多次调用它，只会产生一次影响，但是有相同结果的 HTTP 方法，所以满足幂等性。 HTTP PATCH 方法HTTP PATCH 方法是非幂等的。HTTP POST 方法和 HTTP PUT 方法可能比较好理解，但是 HTTP PATCH 方法只是更新部分资源，怎么是非幂等的呢? 因为，PATCH 提供的实体则需要根据程序或其它协议的定义，解析后在服务器上执行，以此来修改服务器上的资源。换句话说，PATCH 请求是会执行某个程序的，如果重复提交，程序可能执行多次，对服务器上的资源就可能造成额外的影响，这就可以解释它为什么是非幂等的了。 可能你还不能理解这点。我们举个例子 1PATCH /tickets/12 # 更新ticket 12 此时，我们服务端对方法的处理是，当调用一次方法，更新部分字段，将这条 ticket 记录的操作记录加一，这次，每次调用的资源是不是变了呢，所以它是有可能是非幂等的操作。 HTTP DELETE 方法HTTP DELETE 方法用于删除资源，会将资源删除。 1DELETE /tickets/12 # 删除ticekt 12 调用一次和多次对资源产生影响是相同的，所以也满足幂等性。 如何设计符合幂等性的高质量 RESTful APIHTTP GET vs HTTP POST也许，你会想起一个面试题。HTTP 请求的 GET 与 POST 方式有什么区别？ 你可能会回答到：GET 方式通过 URL 提交数据，数据在 URL 中可以看到；POST 方式，数据放置在 HTML HEADER 内提交。但是，我们现在从 RESTful 的资源角度来看待问题，HTTP GET 方法是幂等的，所以它适合作为查询操作，HTTP POST 方法是非幂等的，所以用来表示新增操作。 但是，也有例外，我们有的时候可能需要把查询方法改造成 HTTP POST 方法。比如，超长（1k）的 GET URL 使用 POST 方法来替代，因为 GET 受到 URL 长度的限制。虽然，它不符合幂等性，但是它是一种折中的方案。 HTTP POST vs HTTP PUT对于 HTTP POST 方法和 HTTP PUT 方法，我们一般的理解是 POST 表示创建资源，PUT 表示更新资源。当然，这个是正确的理解。 但是，实际上，两个方法都用于创建资源，更为本质的差别是在幂等性。HTTP POST 方法是非幂等，所以用来表示创建资源，HTTP PUT 方法是幂等的，因此表示更新资源更加贴切。 HTTP PUT vs HTTP PATCH此时，你看会有另外一个问题。HTTP PUT 方法和 HTTP PATCH 方法，都是用来表述更新资源，它们之间有什么区别呢？我们一般的理解是 PUT 表示更新全部资源，PATCH 表示更新部分资源。首先，这个是我们遵守的第一准则。根据上面的描述，PATCH 方法是非幂等的，因此我们在设计我们服务端的 RESTful API 的时候，也需要考虑。如果，我们想要明确的告诉调用者我们的资源是幂等的，我的设计更倾向于使用 HTTP PUT 方法。","categories":[],"tags":[]},{"title":"如何保证接口的幂等性","slug":"73.微服务 - 如何保证接口的幂等性","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:30:03.087Z","comments":true,"path":"2013/07/13/73.微服务 - 如何保证接口的幂等性/","link":"","permalink":"http://example.com/2013/07/13/73.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%B9%82%E7%AD%89%E6%80%A7/","excerpt":"","text":"当通过调用创建实例接口在负载均衡中创建云服务器时，如果遇到了请求超时或服务器内部错误时，客户端可能会尝试重发请求，这时客户端可以通过提供可选参数 ClientToken 避免服务器创建出比预期要多的实例，也就是通过提供 ClientToken 参数保证请求的幂等性。ClientToken 是一个由客户端生成的唯一的、大小写敏感、不超过 64 个 ASCII 字符的字符串。 如果用户使用同一个 ClientToken 值调用创建实例接口，则服务端会返回相同的请求结果，包含相同的 InstanceId。因此用户在遇到错误进行重试的时候，可以通过提供相同的 ClientToken 值，来确保负载均衡只创建一个实例，并得到这个实例的 InstanceId。 如果用户提供了一个已经使用过的 ClientToken，但其他请求参数不同，则负载均衡会返回 IdempotentParameterMismatch 的错误代码。但需要注意的是，SignatureNonce、Timestamp 和 Signature 参数在重试时是需要变化的，因为负载均衡使用 SignatureNonce 来防止重放攻击，使用 Timestamp 来标记每次请求时间，所以再次请求必须提供不同的 SignatureNonce 和 Timestamp 参数值，这同时也会导致 Signature 值的变化。 通常，客户端只需要在 500（InternetError）或 503（ServiceUnavailable）错误、或者无法得到响应结果的情况下进行重试操作。返回结果是 200 时，重试可以得到上次相同的结果，但不会对服务端状态带来任何影响。而对 4xx 的返回错误，通常重试也是不能成功的。","categories":[],"tags":[]},{"title":"说说 CAP 定理、 BASE 理论","slug":"74.微服务 - 说说 CAP 定理、 BASE 理论","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:29:43.192Z","comments":true,"path":"2013/07/13/74.微服务 - 说说 CAP 定理、 BASE 理论/","link":"","permalink":"http://example.com/2013/07/13/74.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E8%AF%B4%E8%AF%B4%20CAP%20%E5%AE%9A%E7%90%86%E3%80%81%20BASE%20%E7%90%86%E8%AE%BA/","excerpt":"","text":"CAP 定理2000 年 7 月，加州大学伯克利分校的 Eric Brewer 教授在 ACM PODC 会议上提出 CAP 猜想。2年后，麻省理工学院的 Seth Gilbert 和 Nancy Lynch 从理论上证明了 CAP。之后，CAP 理论正式成为分布式计算领域的公认定理。 CAP 理论为：一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项。 一致性（Consistency）一致性指 “all nodes see the same data at the same time”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致。 可用性（Availability）可用性指“Reads and writes always succeed”，即服务一直可用，而且是正常响应时间。 分区容错性（Partition tolerance）分区容错性指“the system continues to operate despite arbitrary message loss or failure of part of the system”，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。 CAP 权衡通过 CAP 理论，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？ 对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到 N 个 9，即保证 P 和 A，舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。 对于涉及到钱财这样不能有一丝让步的场景，C 必须保证。网络发生故障宁可停止服务，这是保证 CA，舍弃 P。貌似这几年国内银行业发生了不下 10 起事故，但影响面不大，报到也不多，广大群众知道的少。还有一种是保证 CP，舍弃 A。例如网络故障是只读不写。 孰优孰略，没有定论，只能根据场景定夺，适合的才是最好的。 BASE 理论eBay 的架构师 Dan Pritchett 源于对大规模分布式系统的实践总结，在 ACM 上发表文章提出 BASE 理论，BASE 理论是对 CAP 理论的延伸，核心思想是即使无法做到强一致性（Strong Consistency，CAP 的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。 基本可用（Basically Available）基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。 电商大促时，为了应对访问量激增，部分用户可能会被引导到降级页面，服务层也可能只提供降级服务。这就是损失部分可用性的体现。 软状态（Soft State）软状态是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有三个副本，允许不同节点间副本同步的延时就是软状态的体现。mysql replication 的异步复制也是一种体现。 最终一致性（Eventual Consistency）最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。 ACID 和 BASE 的区别与联系ACID 是传统数据库常用的设计理念，追求强一致性模型。BASE 支持的是大型分布式系统，提出通过牺牲强一致性获得高可用性。 ACID 和 BASE 代表了两种截然相反的设计哲学，在分布式系统设计的场景中，系统组件对一致性要求是不同的，因此 ACID 和 BASE 又会结合使用。","categories":[],"tags":[]},{"title":"怎么考虑数据一致性问题","slug":"75.微服务 - 怎么考虑数据一致性问题","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:29:25.819Z","comments":true,"path":"2013/07/13/75.微服务 - 怎么考虑数据一致性问题/","link":"","permalink":"http://example.com/2013/07/13/75.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E6%80%8E%E4%B9%88%E8%80%83%E8%99%91%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/","excerpt":"","text":"单体应用的数据一致性想象一下如果我们经营着一家大型企业，下属有航空公司、租车公司、和连锁酒店。我们为客户提供一站式的旅游行程规划服务，这样客户只需要提供出行目的地，我们帮助客户预订机票、租车、以及预订酒店。从业务的角度，我们必须保证上述三个服务的预订都完成才能满足一个成功的旅游行程，否则不能成行。 我们的单体应用要满足这个需求非常简单，只需将这个三个服务请求放到同一个数据库事务中，数据库会帮我们保证全部成功或者全部回滚。 当这个功能上线以后，公司非常满意，客户也非常高兴。 微服务场景下的数据一致性这几年中，我们的行程规划服务非常成功，企业蒸蒸日上，用户量也翻了数十倍。企业的下属航空公司、租车公司、和连锁酒店也相继推出了更多服务以满足客户需求，我们的应用和开发团队也因此日渐庞大。如今我们的单体应用已变得如此复杂，以至于没人了解整个应用是怎么运作的。更糟的是新功能的上线现在需要所有研发团队合作，日夜奋战数周才能完成。看着市场占有率每况愈下，公司高层对研发部门越来越不满意。 经过数轮讨论，我们最终决定将庞大的单体应用一分为四：机票预订服务、租车服务、酒店预订服务、和支付服务。服务各自使用自己的数据库，并通过 HTTP 协议通信。负责各服务的团队根据市场需求按照自己的开发节奏发版上线。如今我们面临新的挑战：如何保证最初三个服务的预订都完成才能满足一个成功的旅游行程， 否则不能成行的业务规则？现在服务有各自的边界，而且数据库选型也不尽相同，通过数据库保证数据一致性的方案已不可行。 Sagas幸运的是我们在互联网找到一篇精彩的论文，文中提出的数据一致性解决方案 Saga 恰好满足我们的业务要求。 Saga 是一个长活事务，可被分解成可以交错运行的子事务集合。其中每个子事务都是一个保持数据库一致性的真实事务。 在我们的业务场景下，一个行程规划的事务就是一个 Saga，其中包含四个子事务：机票预订、租车、酒店预订、和支付。 Chris Richardson 在他的文章 Pattern: Saga 中对 Saga 有所描述。 Caitie McCaffrey 也在她的演讲中提到如何在微软的 光晕4 游戏中如何应用 saga 解决数据一致性问题。 Saga 的运行原理 Saga 中的事务相互关联，应作为（非原子）单位执行。任何未完全执行的 Saga 是不满足要求的，如果发生，必须得到补偿。要修正未完全执行的部分，每个 saga 子交易 T1 应提供对应补偿事务 C1 我们根据上述规则定义以下事务及其相应的事务补偿： 服务 事务 补偿 机票预订 预订机票 取消预订 租车 租车 取消预订 酒店预订 预订房间 取消预订 支付 支付 退款 1234当每个 saga 子事务 T1, T2, …, Tn 都有对应的补偿定义 C1, C2, …, Cn-1, 那么 saga 系统可以保证子事务序列 T1, T2, …, Tn 得以完成 (最佳情况)或者序列 T1, T2, …, Tj, Cj, …, C2, C1, 0 &lt; j &lt; n, 得以完成 换句话说，通过上述定义的事务&#x2F;补偿，saga 保证满足以下业务规则： 所有的预订都被执行成功，如果任何一个失败，都会被取消 如果最后一步付款失败，所有预订也将被取消 Saga 的恢复方式原论文中描述了两种类型的 Saga 恢复方式： 向后恢复 补偿所有已完成的事务，如果任一子事务失败 向前恢复 重试失败的事务，假设每个子事务最终都会成功 显然，向前恢复没有必要提供补偿事务，如果你的业务中，子事务（最终）总会成功，或补偿事务难以定义或不可能，向前恢复更符合你的需求。 理论上补偿事务永不失败，然而，在分布式世界中，服务器可能会宕机，网络可能会失败，甚至数据中心也可能会停电。在这种情况下我们能做些什么？最后的手段是提供回退措施，比如人工干预。 使用 Saga 的条件Saga 看起来很有希望满足我们的需求。所有长活事务都可以这样做吗？这里有一些限制： Saga 只允许两个层次的嵌套，顶级的 Saga 和简单子事务 在外层，全原子性不能得到满足。也就是说，sagas 可能会看到其他 sagas 的部分结果 每个子事务应该是独立的原子行为 在我们的业务场景下，航班预订、租车、酒店预订和付款是自然独立的行为，而且每个事务都可以用对应服务的数据库保证原子操作。 我们在行程规划事务层面也不需要原子性。一个用户可以预订最后一张机票，而后由于信用卡余额不足而被取消。同时另一个用户可能开始会看到已无余票，接着由于前者预订被取消，最后一张机票被释放，而抢到最后一个座位并完成行程规划。 补偿也有需考虑的事项： 补偿事务从语义角度撤消了事务 Ti 的行为，但未必能将数据库返回到执行 Ti 时的状态。（例如，如果事务触发导弹发射，则可能无法撤消此操作） 但这对我们的业务来说不是问题。其实难以撤消的行为也有可能被补偿。例如，发送电邮的事务可以通过发送解释问题的另一封电邮来补偿。 现在我们有了通过 Saga 来解决数据一致性问题的方案。它允许我们成功地执行所有事务，或在任何事务失败的情况下，补偿已成功的事务。虽然 Saga 不提供 ACID 保证，但仍适用于许多数据最终一致性的场景。那我们如何设计一个 Saga 系统？ Saga LogSaga 保证所有的子事务都得以完成或补偿，但 Saga 系统本身也可能会崩溃。Saga 崩溃时可能处于以下几个状态： Saga 收到事务请求，但尚未开始。因子事务对应的微服务状态未被 Saga 修改，我们什么也不需要做。 一些子事务已经完成。重启后，Saga 必须接着上次完成的事务恢复。 子事务已开始，但尚未完成。由于远程服务可能已完成事务，也可能事务失败，甚至服务请求超时，saga 只能重新发起之前未确认完成的子事务。这意味着子事务必须幂等。 子事务失败，其补偿事务尚未开始。Saga 必须在重启后执行对应补偿事务。 补偿事务已开始但尚未完成。解决方案与上一个相同。这意味着补偿事务也必须是幂等的。 所有子事务或补偿事务均已完成，与第一种情况相同。 为了恢复到上述状态，我们必须追踪子事务及补偿事务的每一步。我们决定通过事件的方式达到以上要求，并将以下事件保存在名为 saga log 的持久存储中： Saga started event 保存整个 saga 请求，其中包括多个事务&#x2F;补偿请求 Transaction started event 保存对应事务请求 Transaction ended event 保存对应事务请求及其回复 Transaction aborted event 保存对应事务请求和失败的原因 Transaction compensated event 保存对应补偿请求及其回复 Saga ended event 标志着 saga 事务请求的结束，不需要保存任何内容 通过将这些事件持久化在 saga log 中，我们可以将 saga 恢复到上述任何状态。 由于 Saga 只需要做事件的持久化，而事件内容以 JSON 的形式存储，Saga log 的实现非常灵活，数据库（SQL 或 NoSQL），持久消息队列，甚至普通文件可以用作事件存储，当然有些能更快得帮 saga 恢复状态。 Saga 请求的数据结构在我们的业务场景下，航班预订、租车、和酒店预订没有依赖关系，可以并行处理，但对于我们的客户来说，只在所有预订成功后一次付费更加友好。那么这四个服务的事务关系可以用下图表示： 将行程规划请求的数据结构实现为有向非循环图恰好合适。图的根是 saga 启动任务，叶是 saga 结束任务。 Parallel Saga如上所述，航班预订，租车和酒店预订可以并行处理。但是这样做会造成另一个问题：如果航班预订失败，而租车正在处理怎么办？我们不能一直等待租车服务回应，因为不知道需要等多久。 最好的办法是再次发送租车请求，获得回应，以便我们能够继续补偿操作。但如果租车服务永不回应，我们可能需要采取回退措施，比如手动干预。 超时的预订请求可能最后仍被租车服务收到，这时服务已经处理了相同的预订和取消请求。 因此，服务的实现必须保证补偿请求执行以后，再次收到的对应事务请求无效。 Caitie McCaffrey 在她的演讲 Distributed Sagas: A Protocol for Coordinating MicroServices 中把这个称为可交换的补偿请求 (commutative compensating request)。 ACID and SagaACID 是具有以下属性的一致性模型： 原子性（Atomicity） 一致性（Consistency） 隔离性（Isolation） 持久性（Durability） Saga 不提供 ACID 保证，因为原子性和隔离性不能得到满足。原论文描述如下： full atomicity is not provided. That is, sagas may view the partial results of other sagas 通过 saga log，saga 可以保证一致性和持久性。 Saga 架构最后，我们的 Saga 架构如下： Saga Execution Component 解析请求 JSON 并构建请求图 TaskRunner 用任务队列确保请求的执行顺序 TaskConsumer 处理 Saga 任务，将事件写入 saga log，并将请求发送到远程服务 在上文中，我谈到了 ServiceComb 下的 Saga 是怎么设计的。 然而，业界还有其他数据一致性解决方案，如 两阶段提交（2PC） 和 Try-Confirm &#x2F; Cancel（TCC）。那 saga 相比之下有什么特别？ 两阶段提交 Two-Phase Commit (2PC) 两阶段提交协议是一种分布式算法，用于协调参与分布式原子事务的所有进程，以保证他们均完成提交或中止（回滚）事务。 2PC 包含两个阶段： 投票阶段 协调器向所有服务发起投票请求，服务回答 yes 或 no。如果有任何服务回复 no 以拒绝或超时，协调器则在下一阶段发送中止消息。 决定阶段 如果所有服务都回复 yes，协调器则向服务发送 commit 消息，接着服务告知事务完成或失败。如果任何服务提交失败， 协调器将启动额外的步骤以中止该事务。 在投票阶段结束之后与决策阶段结束之前，服务处于不确定状态，因为他们不确定交易是否继续进行。当服务处于不确定状态并与协调器失去连接时，它只能选择等待协调器的恢复，或者咨询其他在确定状态下的服务来得知协调器的决定。在最坏的情况下，n 个处于不确定状态的服务向其他 n-1 个服务咨询将产生 O(n2) 个消息。 另外，2PC 是一个阻塞协议。服务在投票后需要等待协调器的决定，此时服务会阻塞并锁定资源。由于其阻塞机制和最差时间复杂度高，2PC 不能适应随着事务涉及的服务数量增加而扩展的需要。 Try-Confirm&#x2F;Cancel (TCC)TCC 也是补偿型事务模式，支持两阶段的商业模型。 尝试阶段 将服务置于待处理状态。例如，收到尝试请求时，航班预订服务将为客户预留一个座位，并在数据库插入客户预订记录，将记录设为预留状态。如果任何服务失败或超时，协调器将在下一阶段发送取消请求。 确认阶段 将服务设为确认状态。确认请求将确认客户预订的座位，这时服务已可向客户收取机票费用。数据库中的客户预订记录也会被更新为确认状态。如果任何服务无法确认或超时，协调器将重试确认请求直到成功，或在重试了一定次数后采取回退措施，比如人工干预。 与 saga 相比，TCC 的优势在于，尝试阶段将服务转为待处理状态而不是最终状态，这使得设计相应的取消操作轻而易举。 例如，电邮服务的尝试请求可将邮件标记为准备发送，并且仅在确认后发送邮件，其相应的取消请求只需将邮件标记为已废弃。但如果使用 saga，事务将发送电子邮件，及其相应的补偿事务可能需要发送另一封电子邮件作出解释。 TCC 的缺点是其两阶段协议需要设计额外的服务待处理状态，以及额外的接口来处理尝试请求。另外，TCC 处理事务请求所花费的时间可能是 saga 的两倍，因为 TCC 需要与每个服务进行两次通信，并且其确认阶段只能在收到所有服务对尝试请求的响应后开始。 事件驱动的架构和 TCC 一样，在事件驱动的架构中，长活事务涉及的每个服务都需要支持额外的待处理状态。接收到事务请求的服务会在其数据库中插入一条新的记录，将该记录状态设为待处理并发送一个新的事件给事务序列中的下一个服务。 因为在插入记录后服务可能崩溃，我们无法确定是否新事件已发送，所以每个服务还需要额外的事件表来跟踪当前长活事务处于哪一步。 一旦长活事务中的最后一个服务完成其子事务，它将通知它在事务中的前一个服务。接收到完成事件的服务将其在数据库中的记录状态设为完成。 如果仔细比较，事件驱动的架构就像非集中式的基于事件的 TCC 实现。如果去掉待处理状态而直接把服务记录设为最终状态，这个架构就像非集中式的基于事件的 saga 实现。去中心化能达到服务自治，但也造成了服务之间更紧密的的耦合。假设新的业务需求在服务 B 和 C 之间的增加了新的流程 D。在事件驱动架构下，服务 B 和 C 必须改动代码以适应新的流程 D。 Saga 则正好相反，所有这些耦合都在 saga 系统中，当在长活事务中添加新流程时，现有服务不需要任何改动。 集中式与非集中式实现这个 Saga 系列的文章讨论的都是集中式的 saga 设计。但 saga 也可用非集中式的方案来实现。那么非集中式的版本有什么不同？ 非集中式 saga 没有专职的协调器。启动下一个服务调用的服务就是当前的协调器。例如： 服务 A 收到要求服务 A，B 和 C 之间的数据一致性的事务请求。 A 完成其子事务，并将请求传递给事务中的下一个服务，服务 B. B 完成其子事务，并将请求传递给 C，依此类推。 如果 C 处理请求失败，B 有责任启动补偿事务，并要求 A 回滚。 与集中式相比，非集中式的实现具有服务自治的优势。但每个服务都需要包含数据一致性协议，并提供其所需的额外持久化设施。 我们更倾向于自治的业务服务，但服务还关联很多应用的复杂性，如数据一致性，服务监控和消息传递，将这些棘手问题集中处理，能将业务服务从应用的复杂性中释放，专注于处理复杂的业务，因此我们采用了集中式的 saga 设计。 另外，随着长活事务中涉及的服务数量增长，服务之间的关系变得越来越难理解。 总结本文将 saga 与其他数据一致性解决方案进行了比较。Saga 比两阶段提交更易扩展。在事务可补偿的情况下，相比 TCC，saga 对业务逻辑几乎没有改动的需要，而且性能更高。集中式的 saga 设计解耦了服务与数据一致性逻辑及其持久化设施，并使排查事务中的问题更容易。","categories":[],"tags":[]},{"title":"微服务与 SOA 的区别","slug":"77.微服务 - 微服务与 SOA 的区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:26:59.093Z","comments":true,"path":"2013/07/13/77.微服务 - 微服务与 SOA 的区别/","link":"","permalink":"http://example.com/2013/07/13/77.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%8E%20SOA%20%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"微服务是 SOA 发展出来的产物，它是一种比较现代化的细粒度的 SOA 实现方式。 较早实践微服务的公司 Netflix 就曾经称他们构建的架构是「细粒度的 SOA」。 讨论「微服务和 SOA 的差别」的意义远不如讨论「微服务和单体系统的差别」更大，因为他们的区别实在有点微妙。此外，互联网近些年的发展，越来越朝去中心化的方向前进了，就像今天的IT工程师不需要像律师、教师那样，需要得到某些机构的认可才能更好的开展工作，这一方面意味着门槛的降低，另一方面也意味着更多的概念没有一个权威的声音来对它进行定义，使得每个人可以根据自己的需求做出不同的调整。 微服务和 SOA 都是这样背景下的产物，并没有一个权威的定义，来说明它们各自包含了什么东西，使用什么的方法进行系统的构建。但是，还是可以从最大的范围来对比它们的不同，当我们今天说出这两个概念时，其区别往往没有那么大，但 SOA 是有一定的历史了，在历史上的 SOA 往往意味着更多的东西，而这些是现在很多人在做架构设计时不会采用的。","categories":[],"tags":[]},{"title":"说说最终一致性的实现方案","slug":"76.微服务 - 说说最终一致性的实现方案","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:27:17.415Z","comments":true,"path":"2013/07/13/76.微服务 - 说说最终一致性的实现方案/","link":"","permalink":"http://example.com/2013/07/13/76.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E8%AF%B4%E8%AF%B4%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%A1%88/","excerpt":"","text":"问题的起源在电商等业务中，系统一般由多个独立的服务组成，如何解决分布式调用时候数据的一致性？ 具体业务场景如下，比如一个业务操作，如果同时调用服务 A、B、C，需要满足要么同时成功；要么同时失败。A、B、C 可能是多个不同部门开发、部署在不同服务器上的远程服务。 在分布式系统来说，如果不想牺牲一致性，CAP 理论告诉我们只能放弃可用性，这显然不能接受。为了便于讨论问题，先简单介绍下数据一致性的基础理论。 强一致当更新操作完成之后，任何多个后续进程或者线程的访问都会返回最新的更新过的值。这种是对用户最友好的，就是用户上一次写什么，下一次就保证能读到什么。根据 CAP 理论，这种实现需要牺牲可用性。 弱一致性系统并不保证续进程或者线程的访问都会返回最新的更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。 最终一致性弱一致性的特定形式。系统保证在没有后续更新的前提下，系统最终返回上一次更新操作的值。在没有故障发生的前提下，不一致窗口的时间主要受通信延迟，系统负载和复制副本的个数影响。DNS 是一个典型的最终一致性系统。 在工程实践上，为了保障系统的可用性，互联网系统大多将强一致性需求转换成最终一致性的需求，并通过系统执行幂等性的保证，保证数据的最终一致性。但在电商等场景中，对于数据一致性的解决方法和常见的互联网系统（如 MySQL 主从同步）又有一定区别，群友的讨论分成以下 6 种解决方案。 1. 规避分布式事务——业务整合业务整合方案主要采用将接口整合到本地执行的方法。拿问题场景来说，则可以将服务 A、B、C 整合为一个服务 D 给业务，这个服务 D 再通过转换为本地事务的方式，比如服务 D 包含本地服务和服务 E，而服务 E 是本地服务 A ~ C 的整合。 优点：解决（规避）了分布式事务。 缺点：显而易见，把本来规划拆分好的业务，又耦合到了一起，业务职责不清晰，不利于维护。 由于这个方法存在明显缺点，通常不建议使用。 2. 经典方案 - eBay 模式此方案的核心是将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。人工重试更多的是应用于支付场景，通过对账系统对事后问题的处理。 消息日志方案的核心是保证服务接口的幂等性。 考虑到网络通讯失败、数据丢包等原因，如果接口不能保证幂等性，数据的唯一性将很难保证。 eBay 方式的主要思路如下。 BASE：一种 ACID 的替代方案此方案是 eBay 的架构师 Dan Pritchett 在 2008 年发表给 ACM 的文章，是一篇解释 BASE 原则，或者说最终一致性的经典文章。文中讨论了 BASE 与 ACID 原则在保证数据一致性的基本差异。 如果 ACID 为分区的数据库提供一致性的选择，那么如何实现可用性呢？ BASE (basically available, soft state, eventually consistent)BASE 的可用性是通过 支持局部故障 而不是系统全局故障来实现的。下面是一个简单的例子：如果将用户分区在 5 个数据库服务器上，BASE 设计鼓励类似的处理方式，一个用户数据库的故障只影响这台特定主机那 20% 的用户。这里不涉及任何魔法，不过它确实可以带来更高的可感知的系统可用性。 文章中描述了一个最常见的场景，如果产生了一笔交易，需要在交易表增加记录，同时还要修改用户表的金额。这两个表属于不同的远程服务，所以就涉及到分布式事务一致性的问题 文中提出了一个经典的解决方法，将主要修改操作以及更新用户表的消息放在 一个本地事务 来完成。同时为了避免重复消费用户表消息带来的问题，达到多次重试的幂等性，增加一个更新记录表 updates_applied 来记录已经处理过的消息。 系统的执行伪代码如下： 12345678910111213141516171819202122232425262728293031Begin transaction Insert into transaction(id, selller_id, buyer_id, amount); Queue message &quot;update user(&#x27;seller&#x27;, selller_id, amount)&quot;; Queue message &quot;update user(&#x27;buyer&#x27;, buyer_id, amount)&quot;;End transactionFor each message in queue Peek message Begin transaction Select count(*) as processed where trans_id = message.trans_id and balance = message.balance and user_id = message.user_id if processed == 0 if message.balance == &quot;seller&quot; Update user set amt_sold = amt_sold + message.amount where id = message.id; Else Update user set amt_bought = amt_bought + message.amount where id = message.id End if Insert int updates_applied (message.trans_id, message.balance, message.user_id); End if End transaction if transaction successful Remove message from queue End ifEnd for 基于以上方法，在第一阶段，通过本地的数据库的事务保障，增加了 transaction 表及消息队列 。 在第二阶段，分别读出消息队列（但不删除），通过判断更新记录表 updates_applied 来检测相关记录是否被执行，未被执行的记录会修改 user 表，然后增加一条操作记录到 updates_applied，事务执行成功之后再删除队列。 通过以上方法，达到了分布式系统的最终一致性。进一步了解 eBay 的方案可以参考文末链接。 3. 去哪儿网分布式事务方案随着业务规模不断地扩大，电商网站一般都要面临拆分之路。就是将原来一个单体应用拆分成多个不同职责的子系统。比如以前可能将面向用户、客户和运营的功能都放在一个系统里，现在拆分为订单中心、代理商管理、运营系统、报价中心、库存管理等多个子系统。 拆分首先要面临的是什么呢？ 最开始的单体应用所有功能都在一起，存储也在一起。比如运营要取消某个订单，那直接去更新订单表状态，然后更新库存表就 ok 了。因为是单体应用，库在一起，这些都可以在一个事务里，由关系数据库来保证一致性。 但拆分之后就不同了，不同的子系统都有自己的存储。比如订单中心就只管理自己的订单库，而库存管理也有自己的库。那么运营系统取消订单的时候就是通过接口调用等方式来调用订单中心和库存管理的服务了，而不是直接去操作库。这就涉及一个『分布式事务』的问题。 分布式事务有两种解决方式 优先使用异步消息上文已经说过，使用异步消息 Consumer 端需要实现幂等。 幂等有两种方式，一种方式是业务逻辑保证幂等。比如接到支付成功的消息订单状态变成支付完成，如果当前状态是支付完成，则再收到一个支付成功的消息则说明消息重复了，直接作为消息成功处理。 另外一种方式如果业务逻辑无法保证幂等，则要增加一个去重表或者类似的实现。对于 producer 端在业务数据库的同实例上放一个消息库，发消息和业务操作在同一个本地事务里。发消息的时候消息并不立即发出，而是向消息库插入一条消息记录，然后在事务提交的时候再异步将消息发出，发送消息如果成功则将消息库里的消息删除，如果遇到消息队列服务异常或网络问题，消息没有成功发出那么消息就留在这里了，会有另外一个服务不断地将这些消息扫出重新发送。 有的业务不适合异步消息的方式，事务的各个参与方都需要同步的得到结果这种情况的实现方式其实和上面类似，每个参与方的本地业务库的同实例上面放一个事务记录库。 比如 A 同步调用 B，C。A 本地事务成功的时候更新本地事务记录状态，B 和 C 同样。如果有一次 A 调用 B 失败了，这个失败可能是 B 真的失败了，也可能是调用超时，实际 B 成功。则由一个中心服务对比三方的事务记录表，做一个最终决定。假设现在三方的事务记录是 A 成功，B 失败，C 成功。那么最终决定有两种方式，根据具体场景： 重试 B，直到 B 成功，事务记录表里记录了各项调用参数等信息； 执行 A 和 B 的补偿操作(一种可行的补偿方式是回滚)。 对 b 场景做一个特殊说明：比如 B 是扣库存服务，在第一次调用的时候因为某种原因失败了，但是重试的时候库存已经变为 0，无法重试成功，这个时候只有回滚 A 和 C 了。 那么可能有人觉得在业务库的同实例里放消息库或事务记录库，会对业务侵入，业务还要关心这个库，是否一个合理的设计？ 实际上可以依靠运维的手段来简化开发的侵入，我们的方法是让 DBA 在公司所有 MySQL 实例上预初始化这个库，通过框架层（消息的客户端或事务 RPC 框架）透明的在背后操作这个库，业务开发人员只需要关心自己的业务逻辑，不需要直接访问这个库。 总结起来，其实两种方式的根本原理是类似的，也就是将分布式事务转换为多个本地事务，然后依靠重试等方式达到最终一致性。 4. 蘑菇街交易创建过程中的分布式一致性方案交易创建的一般性流程我们把交易创建流程抽象出一系列可扩展的功能点，每个功能点都可以有多个实现（具体的实现之间有组合&#x2F;互斥关系）。把各个功能点按照一定流程串起来，就完成了交易创建的过程。 面临的问题每个功能点的实现都可能会依赖外部服务。那么如何保证各个服务之间的数据是一致的呢？比如锁定优惠券服务调用超时了，不能确定到底有没有锁券成功，该如何处理？再比如锁券成功了，但是扣减库存失败了，该如何处理？ 方案选型服务依赖过多，会带来管理复杂性增加和稳定性风险增大的问题。试想如果我们强依赖 10 个服务，9 个都执行成功了，最后一个执行失败了，那么是不是前面 9 个都要回滚掉？这个成本还是非常高的。 所以在拆分大的流程为多个小的本地事务的前提下，对于非实时、非强一致性的关联业务写入，在本地事务执行成功后，我们选择发消息通知、关联事务异步化执行的方案。 消息通知往往不能保证 100% 成功；且消息通知后，接收方业务是否能执行成功还是未知数。前者问题可以通过重试解决；后者可以选用事务消息来保证。 但是事务消息框架本身会给业务代码带来侵入性和复杂性，所以我们选择基于 DB 事件变化通知到 MQ 的方式做系统间解耦，通过订阅方消费 MQ 消息时的 ACK 机制，保证消息一定消费成功，达到最终一致性。由于消息可能会被重发，消息订阅方业务逻辑处理要做好幂等保证。 所以目前只剩下需要实时同步做、有强一致性要求的业务场景了。在交易创建过程中，锁券和扣减库存是这样的两个典型场景。 要保证多个系统间数据一致，乍一看，必须要引入分布式事务框架才能解决。但引入非常重的类似二阶段提交分布式事务框架会带来复杂性的急剧上升；在电商领域，绝对的强一致是过于理想化的，我们可以选择准实时的最终一致性。 我们在交易创建流程中，首先创建一个不可见订单，然后在同步调用锁券和扣减库存时，针对调用异常（失败或者超时），发出废单消息到MQ。如果消息发送失败，本地会做时间阶梯式的异步重试；优惠券系统和库存系统收到消息后，会进行判断是否需要做业务回滚，这样就准实时地保证了多个本地事务的最终一致性。 5. 支付宝及蚂蚁金融云的分布式服务 DTS 方案业界常用的还有支付宝的一种 xts 方案，由支付宝在 2PC 的基础上改进而来。主要思路如下，大部分信息引用自官方网站。 分布式事务服务简介分布式事务服务 (Distributed Transaction Service, DTS) 是一个分布式事务框架，用来保障在大规模分布式环境下事务的最终一致性。DTS 从架构上分为 xts-client 和 xts-server 两部分，前者是一个嵌入客户端应用的 JAR 包，主要负责事务数据的写入和处理；后者是一个独立的系统，主要负责异常事务的恢复。 核心特性传统关系型数据库的事务模型必须遵守 ACID 原则。在单数据库模式下，ACID 模型能有效保障数据的完整性，但是在大规模分布式环境下，一个业务往往会跨越多个数据库，如何保证这多个数据库之间的数据一致性，需要其他行之有效的策略。在 JavaEE 规范中使用 2PC (2 Phase Commit, 两阶段提交) 来处理跨 DB 环境下的事务问题，但是 2PC 是反可伸缩模式，也就是说，在事务处理过程中，参与者需要一直持有资源直到整个分布式事务结束。这样，当业务规模达到千万级以上时，2PC 的局限性就越来越明显，系统可伸缩性会变得很差。基于此，我们采用 BASE 的思想实现了一套类似 2PC 的分布式事务方案，这就是 DTS。DTS在充分保障分布式环境下高可用性、高可靠性的同时兼顾数据一致性的要求，其最大的特点是保证数据最终一致 (Eventually consistent)。 简单的说，DTS 框架有如下特性： 最终一致：事务处理过程中，会有短暂不一致的情况，但通过恢复系统，可以让事务的数据达到最终一致的目标。 协议简单：DTS 定义了类似 2PC 的标准两阶段接口，业务系统只需要实现对应的接口就可以使用 DTS 的事务功能。 与 RPC 服务协议无关：在 SOA 架构下，一个或多个 DB 操作往往被包装成一个一个的 Service，Service 与 Service 之间通过 RPC 协议通信。DTS 框架构建在 SOA 架构上，与底层协议无关。 与底层事务实现无关： DTS 是一个抽象的基于 Service 层的概念，与底层事务实现无关，也就是说在 DTS 的范围内，无论是关系型数据库 MySQL，Oracle，还是 KV 存储 MemCache，或者列存数据库 HBase，只要将对其的操作包装成 DTS 的参与者，就可以接入到 DTS 事务范围内。 以下是分布式事务框架的流程图 实现 一个完整的业务活动由一个主业务服务与若干从业务服务组成。 主业务服务负责发起并完成整个业务活动。 从业务服务提供 TCC 型业务操作。 业务活动管理器控制业务活动的一致性，它登记业务活动中的操作，并在活动提交时确认所有的两阶段事务的 confirm 操作，在业务活动取消时调用所有两阶段事务的 cancel 操作。” 与 2PC 协议比较 没有单独的 Prepare 阶段，降低协议成本 系统故障容忍度高，恢复简单 6. 农信网数据一致性方案电商业务公司的支付部门，通过接入其它第三方支付系统来提供支付服务给业务部门，支付服务是一个基于 Dubbo 的 RPC 服务。 对于业务部门来说，电商部门的订单支付，需要调用 支付平台的支付接口来处理订单； 同时需要调用积分中心的接口，按照业务规则，给用户增加积分。 从业务规则上需要同时保证业务数据的实时性和一致性，也就是支付成功必须加积分。 我们采用的方式是同步调用，首先处理本地事务业务。考虑到积分业务比较单一且业务影响低于支付，由积分平台提供增加与回撤接口。 具体的流程是先调用积分平台增加用户积分，再调用支付平台进行支付处理，如果处理失败，catch 方法调用积分平台的回撤方法，将本次处理的积分订单回撤。 用户信息变更公司的用户信息，统一由用户中心维护，而用户信息的变更需要同步给各业务子系统，业务子系统再根据变更内容，处理各自业务。用户中心作为 MQ 的 producer，添加通知给 MQ。APP Server 订阅该消息，同步本地数据信息，再处理相关业务比如 APP 退出下线等。 我们采用异步消息通知机制，目前主要使用 ActiveMQ，基于 Virtual Topic 的订阅方式，保证单个业务集群订阅的单次消费。 总结分布式服务对衍生的配套系统要求比较多，特别是我们基于消息、日志的最终一致性方案，需要考虑消息的积压、消费情况、监控、报警等。","categories":[],"tags":[]},{"title":"如何拆分服务","slug":"78.微服务 - 如何拆分服务","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:26:33.837Z","comments":true,"path":"2013/07/13/78.微服务 - 如何拆分服务/","link":"","permalink":"http://example.com/2013/07/13/78.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E5%A6%82%E4%BD%95%E6%8B%86%E5%88%86%E6%9C%8D%E5%8A%A1/","excerpt":"","text":"如今，市场环境纷繁复杂，瞬息万变，现代企业为了更好地生存，需要有极强的适应能力。快速而轻松地迎接改变，成为了一个优质企业的特征之一，同时企业还要求技术团队构建更科学的架构，搭建成本更低的平台，这就使得这些团队越来越倾向于使用微服务架构来应对以上要求。 微服务的做法有利于软件组件和数据的分散化，将一个整体分解成更小、更容易改变的部分，分散仅帮助团队加快工程进度，而不会牺牲系统的安全性。要想让这种架构工作得很好，需要改变工作方式。 微服务架构的设计，其实是为了使团队能够在执行工作的人之间分配决策权力，向更多成员直接推行决策权，允许他们以更自由的方式生产。微服务架构使用正确的话，将产生更好和更快的变化。但是如果你的架构错误，那么一系列坏的决定可能会降低转化率，甚至会损害你的业务。 我们讲决策权分配，即是说微服务架构的拆分实际上就是在寻求正确的权力下放战略。这是一个进化过程，需要不断地进行分析和调整。而如何正确的拆分微服务架构，我认为可以重点从以下三个方面考虑： 1. 我们应该做哪些决定？设计微服务系统不仅仅是改变组件大小，架构中涉及创建和更改服务的所有领域都有一定的作用。在这里总结了以下九个方面，作为拆分微服务架构时所做决定的参考： 生命周期：什么时候创建或停止服务？我们什么时候需要将它们分开？ 服务实现：我们应该在每个服务中使用哪些工具、语言和架构？ 系统架构：服务如何引导他人？开发人员如何了解？ 数据架构：服务之间如何共享数据？ 变更过程：什么时候可以改变服务？部署和 QA 的工具和过程？ 团队管理：谁在哪个团队服务？每个团队负责什么？团队成员做了什么？ 人事管理：人员如何被雇用和解雇？员工如何激励和奖励？ 安全管理：我们如何降低安全事故的风险？需要做些什么来改善整个系统的安全性？ 采购过程：可以购买什么软件？使用开源软件需要哪些保护？ 2. 涉及到哪些人？不得不承认，一个员工做出的一些决定对他们的公司来说可能是非常有影响力的。一个很典型的现象是，公司试图对内部的决策者增加控制，以便将风险降至最低，从而导致决策权集中化。例如，在过去的几年中，苹果以拥有一个高度集中的设计团队而闻名，少数的人做出产品设计的大部分决策。 集中发生是因为正确的人需要做出最重要的决定。通常，“正确的人”是具有天赋，专业知识和经验的组合，使我们能够相信他们做出最好的决定，我们可以称这些人为我们的“明星决策者”。但实际上，公司的“明星决策者”数量有限，大多数团队只有几颗“明星”。 微服务改变了这种少数人行使决策权的现象，同时更容易应对错误的决策。如果一个团队在微服务工作时作出错误的决定，其错误的波及范围会很小，容错率变高。当系统的更改变得便宜和容易时，团队可以快速改进先前的决策，使他们能够更快地获得最佳决策。 3. 谁拥有哪一部分？决策基于选择，而选择又基于领域知识。决策不应该立即执行，它需要一个过程，需要高度专业的技能或知识来实现。 管理专家亨利•明茨伯格（Henry Mintzberg）为我们提供了一个很好的模式，其中概述了决策过程的步骤： 研究与信息采集 生成选择 做出选择 授权的选择 执行和实施 所有这一切的关键，是在采用决策分权化时，不需要绝对。每个步骤都可以独立集中或分散化，在平衡效率和安全性时，可以获得更大的灵活性。如今，集中选择与非集中选择相结合，也是现在很多大公司常见的模式。 当人们谈论微服务架构时，权力下放过程的选择、授权和执行部分相互协作、快速、规模化地移动，这些都降低了变化来临时对整体系统产生的负面影响。它是提高变革速度的有效途径，但不要忘记，你的成员特点、团队协调，以及所有的系统、工具和工作环境同样重要。 你必须了解如何做出决策，如何改进流程，这才是迎接变化的好方法。","categories":[],"tags":[]},{"title":"int和Integer有什么区别","slug":"8.Java 基础 - int 和 Integer 有什么区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-28T09:52:45.384Z","comments":true,"path":"2013/07/13/8.Java 基础 - int 和 Integer 有什么区别/","link":"","permalink":"http://example.com/2013/07/13/8.Java%20%E5%9F%BA%E7%A1%80%20-%20int%20%E5%92%8C%20Integer%20%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/","excerpt":"","text":"int 是 Java 提供的 8 种原始数据类型之一。Java 为每个原始类型提供了封装类，Integer 是 Java 为 int 提供的封装类。 int 的默认值为 0，而 Integer 的默认值为 null，是引用类型，即 Integer 可以区分出未赋值和值为 0 的区别，int 则无法表达出未赋值的情况， Java 中 int 和 Integer 关系是比较微妙的。关系如下： int 是基本的数据类型； Integer 是 int 的封装类； int 和 Integer 都可以表示某一个数值； int 和 Integer 不能够互用，因为他们两种不同的数据类型；","categories":[],"tags":[]},{"title":"微服务如何进行数据库管理","slug":"79.微服务 - 微服务如何进行数据库管理","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:25:40.633Z","comments":true,"path":"2013/07/13/79.微服务 - 微服务如何进行数据库管理/","link":"","permalink":"http://example.com/2013/07/13/79.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86/","excerpt":"","text":"分布式数据管理之痛点为了确保微服务之间松耦合，每个服务都有自己的数据库, 有的是关系型数据库(SQL)，有的是非关系型数据库(NoSQL)。开发企业事务往往牵涉到多个服务，要想做到多个服务数据的一致性并非易事，同样，在多个服务之间进行数据查询也充满挑战。我们以一个在线 B2B 商店为例，客户服务包括了客户的各种信息，例如可用信用等。管理订单，提供订单服务，则需要验证某个新订单与客户的信用限制没有冲突。在单体应用中，订单服务只需要使用传统事务交易就可以一次性检查可用信用和创建订单。相反微服务架构下，订单和客户表分别是相应服务的私有表，如下图所示： 订单服务不能直接访问客户表，只能通过客户服务发布的 API 来访问或者使用分布式事务, 也就是众所周知的两阶段提交 (2PC)来访问客户表，2PC 意义图如下所示： 这里存在两个挑战： 第一个挑战是 2PC 除要求数据库本身支持外，还要求服务的数据库类型需要保持一致。但是现在的微服务架构中，每个服务的数据库类型可能是不一样的，有的可能是 MySQL 数据库，有的也可能是 NoSQL 数据库; 第二个挑战是如何实现从多个服务中查询数据。假设应用程序需要显示一个客户和他最近的订单。如果订单服务提供用于检索客户订单的 API，那么应用程序端可以通过 JOIN 方式来检索此数据，即应用程序首选从客户服务检索客户，并从订单服务检索客户的订单。然而，如果订单服务仅支持通过其主键查找订单(也许它使用仅支持基于主键的检索的 NoSQL 数据库)， 在这种情况下，就没有方法来检索查询所需的数据。 为解决这两大痛点，就需要我们使用到分步式数据管理了。 分布式数据管理之举措在介绍分布式数据管理(CRUD)解决方案之前，有必要介绍下 CAP 原理和最终一致性相关概念。 CAP 原理(CAP Theorem)在足球比赛里，一个球员在一场比赛中进三个球，称之为帽子戏法(Hat-trick)。在分布式数据系统中，也有一个帽子原理(CAP Theorem)，不过此帽子非彼帽子。CAP 原理中，有三个要素： 一致性(C onsistency) 可用性(A vailability) 分区容忍性(P artition tolerance) CAP 原理指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。 因此在进行分布式架构设计时，必须做出取舍。而对于分布式数据系统，分区容忍性是基本要求 ，否则就失去了价值，因此设计分布式数据系统，就是在一致性和可用性之间取一个平衡。 对于大多数 WEB 应用，其实并不需要强一致性，因此牺牲一致性而换取高可用性，是目前多数分布式数据库产品的方向。 当然，牺牲一致性，并不是完全不管数据的一致性，否则数据是混乱的，那么系统可用性再高分布式再好也没有了价值。 牺牲一致性，只是不再要求关系型数 据库中的强一致性，而是只要系统能达到最终一致性即可，考虑到客户体验，这个最终一致的时间窗口，要尽可能的对用户透明，也就是需要保障“用户感知到的一致性”。 通常是通过数据的多份异步复制来实现系统的高可用和数据的最终一致性的，“用户感知到的一致性”的时间窗口则 取决于数据复制到一致状态的时间。 最终一致性(eventually consistent)对于一致性，可以分为从客户端和服务端两个不同的视角。 从客户端来看，一致性主要指的是多并发访问时更新过的数据如何获取的问题。 从服务端来看，则是更新如何复制分布到整个系统，以保证数据最终一致。 一致性是因为有并发读写才有的问题，因此在理解一致性的问题时，一定要注意结合考虑并发读写的场景。 从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。 对于关系型数据库，要求更新过的数据能被后续的 访问都能看到，这是强一致性 ;如果能容忍后续的部分或者全部访问不到，则是弱一致性 ; 如果经过一段时间后要求能访问到更新后的数据，则是最终一致性。 从服务端角度，如何尽快将更新后的数据分布到整个系统，降低达到最终一致性的时间窗口，是提高系统的可用度和用户体验非常重要的方面。 那么问题来了，如何实现数据的最终一致性呢?答案就在事件驱动架构。 事件驱动架构简介Chris Richardson 作为微服务架构设计领域的权威，给出了分布式数据管理的最佳解决方案。 对于大多数应用而言，要实现微服务的分布式数据管理，需要采用事件驱动架构(event-driven architecture)。 在事件驱动架构中，当某件重要事情发生时，微服务会发布一个事件，例如更新一个业务实体。 当订阅这些事件的微服务接收此事件时，就可以更新自己的业务实体，也可能会引发更多的事件发布，让其他相关服务进行数据更新，最终实现分布式数据最终一致性。 可以使用事件来实现跨多服务的业务交易。交易一般由一系列步骤构成，每一步骤都由一个更新业务实体的微服务和发布激活下一步骤的事件构成。 事件驱动示例 1下图展现如何使用事件驱动方法，在创建订单时检查信用可用度，微服务之间通过消息代理(Messsage Broker)来交换事件。 订单服务创建一个带有 NEW 状态的 Order (订单)，发布了一个“Order Created Event(创建订单)”的事件。 客户服务消费 Order Created Event 事件，为此订单预留信用，发布“Credit Reserved Event(信用预留)”事件。 订单服务消费 Credit Reserved Event，改变订单的状态为 OPEN。 事件驱动示例 2下图展现如何使用事件驱动方法，在创建订单时触发支付业务的数据更新，微服务之间通过消息代理(Messsage Broker)来交换事件。 订单服务创建一个待支付的订单，发布一个“创建订单”的事件。 支付服务消费“创建订单”事件，支付完成后发布一个“支付完成”事件。 订单服务消费“支付完成”事件，订单状态更新为待出库。 事件驱动架构之分布式数据更新上节通过示例概要介绍了通过事件驱动方式，实现了分布式数据最终一致性保证。纵观微服务架构下的事件驱动业务处理逻辑，其核心要点在于，可靠的事件投递和避免事件的重复消费。 可靠事件投递有以下两个特性： 每个服务原子性的完成业务操作和发布事件; 消息代理确保事件投递至少一次(at least once); 而避免事件重复消费则要求消费事件的服务实现幂等性，比如支付服务不能因为重复收到事件而多次支付。 BTW：当前流行的消息队列如Kafka等，都已经实现了事件的持久化和at least once的投递模式，所以可靠事件投递的第二条特性已经满足，这里就不展开。接下来章节讲重点讲述如何实现可靠事件投递的第一条特性和避免事件重复消费，即服务的业务操作和发布事件的原子性和避免消费者重复消费事件要求服务实现幂等性。 如何实现事件投递操作原子性?事件驱动架构会碰到数据库更新和发布事件原子性问题。例如，订单服务必须向 ORDER 表插入一行，然后发布 Order Created event，这两个操作需要原子性。比如更新数据库后，服务瘫了(crashes)造成事件未能发布，系统变成不一致状态。那么如何实现服务的业务操作和发布事件的原子性呢? 使用本地事务发布事件 获得原子性的一个方法是将服务的业务操作和发布事件放在一个本地数据库事务里，也就是说，需要在本地建立一个 EVENT 表，此表在存储业务实体数据库中起到消息列表功能。当应用发起一个(本地)数据库交易，更新业务实体状态时，会向 EVENT 表中插入一个事件，然后提交此次交易。另外一个独立应用进程或者线程查询此 EVENT 表，向消息代理发布事件，然后使用本地交易标志此事件为已发布，如下图所示： 订单服务向 ORDER 表插入一行，然后向 EVENT 表中插入 Order Created event，事件发布线程或者进程查询 EVENT 表，请求未发布事件，发布他们，然后更新 EVENT 表标志此事件为已发布。 此方法也是优缺点都有。优点是可以确保事件发布不依赖于 2PC，应用发布业务层级事件而不需要推断他们发生了什么;而缺点在于此方法由于开发人员必须牢记发布事件，因此有可能出现错误。 使用事件源 Event sourcing (事件源)通过使用以事件中心的数据存储方式来保证业务实体的一致性。事件源保存了每个业务实体所有状态变化的事件，而不是存储实体当前的状态。应用可以通过重放事件来重建实体现在的状态。只要业务实体发生变化，新事件就会添加到事件表中。因为保存事件是单一操作，因此肯定是原子性的。 为了理解事件源工作方式，考虑以事件实体作为一个例子说明。传统方式中，每个订单映射为 ORDER 表中一行。但是对于事件源方式，订单服务以事件状态改变方式存储一个订单：创建的，已批准的，已发货的，取消的;每个事件包括足够信息来重建订单的状态。 事件源方法有很多优点：解决了事件驱动架构关键问题，使得业务实体更新和事件发布原子化，但是也存在缺点，因为是持久化事件而不是对象，导致数据查询时，必须使用 Command Query Responsibility Segregation (CQRS) 来完成查询业务，从开发角度看，存在一定挑战。 如何避免事件重复消费?要避免事件重复消费，需要消费事件的服务实现服务幂等，因为存在重试和错误补偿机制，不可避免的在系统中存在重复收到消息的场景，服务幂等能提高数据的一致性。在编程中,一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同，因此需要开发人员在功能设计实现时，需要特别注意服务的幂等性。 事件驱动架构之分布式数据查询微服务架构下，由于分布式数据库的存在，导致在执行用户业务数据查询时，通常需要跨多个微服务数据库进行数据查询，也就是分布式数据查询。那么问题来了，由于每个微服务的数据都是私有化的，只能通过各自的REST接口获取，如果负责业务查询的功能模块，通过调用各个微服务的REST接口来分别获取基础数据，然后在内存中再进行业务数据拼装后，再返回给用户。该方法无论从程序设计或是查询性能角度看，都不是一个很好的方法。那么如何解决微服务架构下的分布式数据查询问题呢? 在给出解决方案之前，需要读者首先了解下物化视图和命令查询职责分离等相关概念。 什么是物化视图(merialized views)?物化视图是包括一个查询结果的数据库对像，它是远程数据的的本地副本，或者用来生成基于数据表求和的汇总表。物化视图存储基于远程表的数据，也可以称为快照。这个基本上就说出了物化视图的本质，它是一组查询的结果，这样势必为将来再次需要这组数据时大大提高查询性能。物化视图有两种刷新模式 ON DEMAND 和 ON COMMIT，用户可根据实际情况进行设置。 物化视图对于应用层是透明的，不需要有任何的改动，终端用户甚至都感觉不到底层是用的物化视图。总之，使用物化视图的目的一个是提高查询性能，另一个是由于物化视图包含的数据是远程数据库的数据快照或拷贝，微服务可通过物化视图和命令查询职责分离(CQRS)技术(参见以下章节)实现分布式数据查询。 什么是命令查询职责分离(CQRS)?在常用的单体应用架构中，通常都是通过数据访问层来修改或者查询数据，一般修改和查询使用的是相同的实体。在一些业务逻辑简单的系统中可能没有什么问题，但是随着系统逻辑变得复杂，用户增多，这种设计就会出现一些性能问题;另外更重要的是，在微服务架构下，通常需要跨多个微服务数据库来查询数据，此时，我们可借助命令查询职责分离(CQRS)来有效解决这些问题。 CQRS 使用分离的接口将数据查询操作(Queries)和数据修改操作(Commands)分离开来，这也意味着在查询和更新过程中使用的数据模型也是不一样的。这样读和写逻辑就隔离开来了。使用 CQRS 分离了读写职责之后，可以对数据进行读写分离操作来改进性能，同时提高可扩展性和安全。如下图： 主数据库处理 CUD，从库处理 R，从库的的结构可以和主库的结构完全一样，也可以不一样，从库主要用来进行只读的查询操作。在数量上从库的个数也可以根据查询的规模进行扩展，在业务逻辑上，也可以根据专题从主库中划分出不同的从库。从库也可以实现成 ReportingDatabase，根据查询的业务需求，从主库中抽取一些必要的数据生成一系列查询报表来存储。 使用 ReportingDatabase 的一些优点通常可以使得查询变得更加简单高效： ReportingDatabase 的结构和数据表会针对常用的查询请求进行设计。 ReportingDatabase 数据库通常会去正规化，存储一些冗余而减少必要的 Join 等联合查询操作，使得查询简化和高效，一些在主数据库中用不到的数据信息，在 ReportingDatabase 可以不用存储。 可以对 ReportingDatabase 重构优化，而不用去改变操作数据库。 对 ReportingDatabase 数据库的查询不会给操作数据库带来任何压力。 可以针对不同的查询请求建立不同的 ReportingDatabase 库。 如何实现事件驱动架构下的数据查询服务?事件驱动不仅可以用于分布式数据一致性保证，还可以借助物化视图和命令查询职责分离技术，使用事件来维护不同微服务拥有数据预连接(pre-join)的物化视图，从而实现微服务架构下的分布式数据查询。维护物化视图的服务订阅了相关事件并在事件发生时更新物化视图。例如，客户订单视图更新服务(维护客户订单视图)会订阅由客户服务和订单服务发布的事件(您还可以使用事件来维护由多个微服务拥有的数据组成的物化视图。 例如上图中间的 “客户订单视图更新”服务，主要负责客户订单视图的更新。该服务订阅了客户服务和订单服务发布的事件。当“客户订单视图更新”服务收到了上图左侧的客户或者订单更新事件，则会触发更新客户订单物化视图数据集。这里可以使用文档数据库(例如 MongoDB )来实现客户订单视图，为每个用户存储一个文档。而上图右侧的客户订单视图查询服务负责响应对客户以及最近订单(通过查询客户订单视图数据集)的查询。 总之，上图所示业务逻辑，用到了事件驱动、物化视图和命令查询职责分离等技术，有效解决了微服务架构下分布式数据查询的问题。 事件驱动架构优缺点事件驱动架构既有优点也有缺点，此架构可以实现跨多个服务的事务实现，且提供最终数据一致性，并且使得服务能够自动维护查询视图;而缺点在于编程模式比传统基于事务的交易模式更加复杂，必须实现补偿事务以便从应用程序级故障中恢复，例如，如果信用检查不成功则必须取消订单;另外，应用必须应对不一致的数据，比如当应用读取未更新的最终视图时也会遇见数据不一致问题。另外一个缺点在于订阅者必须检测和忽略冗余事件，避免事件重复消费。 总结在微服务架构中，每个微服务都有自己私有的数据集。不同微服务可能使用不同的SQL或者NoSQL数据库。尽管数据库架构有很强的优势，但是也面对数据分布式管理的挑战。第一个挑战就是如何在多服务之间维护业务数据一致性;第二个挑战是如何从多服务环境中获取一致性数据。 最佳解决办法是采用事件驱动架构。其中碰到的一个挑战是如何原子性的更新状态和发布事件。有几种方法可以解决此问题，包括将数据库视为消息队列和事件源等。 从目前技术应用范围和成熟度看，推荐使用第一种方式(本地事务发布事件)，来实现事件投递原子化，即可靠事件投递。 重要提醒数据一致性是微服务架构设计中唯恐避之不及却又不得不考虑的话题。通过保证事件驱动实现最终数据的一致性，此方案的优劣，也不能简单的一言而概之，而是应该根据场景定夺，适合的才是最好的。另外，我们在对微服务进行业务划分的时候就尽可能的避免“可能会产生一致性问题”的设计。如果这种设计过多，也许是时候考虑改改设计了。","categories":[],"tags":[]},{"title":"如何应对微服务的链式调用异常","slug":"80.微服务 - 如何应对微服务的链式调用异常","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:25:00.268Z","comments":true,"path":"2013/07/13/80.微服务 - 如何应对微服务的链式调用异常/","link":"","permalink":"http://example.com/2013/07/13/80.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E9%93%BE%E5%BC%8F%E8%B0%83%E7%94%A8%E5%BC%82%E5%B8%B8/","excerpt":"","text":"一般情况下，每个微服务之间是独立的，如果某个服务宕机，只会影响到当前服务，而不会对整个业务系统产生影响。但是，服务端可能会在多个微服务之间产生一条链式调用，并把整合后的信息返回给客户端。在调用过程中，如果某个服务宕机或者网络不稳定可能造成整个请求失败。因此，为了应对微服务的链式调用异常，我们需要在设计微服务调用链时不宜过长，以免客户端长时间等待，以及中间环节出现错误造成整个请求失败。此外，可以考虑使用消息队列进行业务解耦，并且使用缓存避免微服务的链式调用从而提高该接口的可用性。","categories":[],"tags":[]},{"title":"对于快速追踪与定位问题","slug":"81.微服务 - 对于快速追踪与定位问题","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:24:19.802Z","comments":true,"path":"2013/07/13/81.微服务 - 对于快速追踪与定位问题/","link":"","permalink":"http://example.com/2013/07/13/81.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E5%AF%B9%E4%BA%8E%E5%BF%AB%E9%80%9F%E8%BF%BD%E8%B8%AA%E4%B8%8E%E5%AE%9A%E4%BD%8D%E9%97%AE%E9%A2%98/","excerpt":"","text":"在微服务复杂的链式调用中，我们会比单体架构更难以追踪与定位问题。因此，在设计的时候，需要特别注意。一种比较好的方案是，当 RESTful API 接口出现非 2xx 的 HTTP 错误码响应时，采用全局的异常结构响应信息。其中，code 字段用来表示某类错误的错误码，在微服务中应该加上“{biz_name}&#x2F;”前缀以便于定位错误发生在哪个业务系统上。我们来看一个案例，假设“用户中心”某个接口没有权限获取资源而出现错误，我们的业务系统可以响应“UC&#x2F;AUTH_DENIED”，并且通过自动生成的 UUID 值的 request_id 字段，在日志系统中获得错误的详细信息。 12345678910HTTP/1.1 400 Bad RequestContent-Type: application/json&#123; &quot;code&quot;: &quot;INVALID_ARGUMENT&quot;, &quot;message&quot;: &quot;&#123;error message&#125;&quot;, &quot;cause&quot;: &quot;&#123;cause message&#125;&quot;, &quot;request_id&quot;: &quot;01234567-89ab-cdef-0123-456789abcdef&quot;, &quot;host_id&quot;: &quot;&#123;server identity&#125;&quot;, &quot;server_time&quot;: &quot;2014-01-01T12:00:00Z&quot;&#125;","categories":[],"tags":[]},{"title":"微服务的安全","slug":"82.微服务 - 微服务的安全","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-24T06:23:29.942Z","comments":true,"path":"2013/07/13/82.微服务 - 微服务的安全/","link":"","permalink":"http://example.com/2013/07/13/82.%E5%BE%AE%E6%9C%8D%E5%8A%A1%20-%20%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9A%84%E5%AE%89%E5%85%A8/","excerpt":"","text":"OAuth 是一个关于授权的开放网络标准，它允许第三方网站在用户授权的前提下访问用户在服务商那里存储的各种信息。实际上，OAuth 2.0 允许用户提供一个令牌给第三方网站，一个令牌对应一个特定的第三方网站，同时该令牌只能在特定的时间内访问特定的资源。用户在客户端使用用户名和密码在用户中心获得授权，然后客户端在访问应用是附上 Token 令牌。此时，应用接收到客户端的 Token 令牌到用户中心进行认证。 一般情况下，access token 会添加到 HTTP Header 的 Authorization 参数中使用，其中经常使用到的是 Bearer Token 与 Mac Token。其中，Bearer Token 适用于安全的网络下 API 授权。MAC Token 适用于不安全的网络下 API 授权。","categories":[],"tags":[]},{"title":"重载和重写的区别","slug":"9.Java 基础 - 重载和重写的区别","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-28T09:52:45.384Z","comments":true,"path":"2013/07/13/9.Java 基础 - 重载和重写的区别/","link":"","permalink":"http://example.com/2013/07/13/9.Java%20%E5%9F%BA%E7%A1%80%20-%20%E9%87%8D%E8%BD%BD%E5%92%8C%E9%87%8D%E5%86%99%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"重载 Overload表示同一个类中可以有多个名称相同的方法，但这些方法的参数列表各不相同（即参数个数或类型不同）。 重写 Override表示子类中的方法可以与父类中的某个方法的名称和参数完全相同，通过子类创建的实例对象调用这个方法时，将调用子类中的定义方法，这相当于把父类中定义的那个完全相同的方法给覆盖了，这也是面向对象编程的多态性的一种表现。子类覆盖父类的方法时，只能比父类抛出更少的异常，或者是抛出父类抛出的异常的子异常，因为子类可以解决父类的一些问题，不能比父类有更多的问题。子类方法的访问权限只能比父类的更大，不能更小。如果父类的方法是private类型，那么，子类则不存在覆盖的限制，相当于子类中增加了一个全新的方法","categories":[],"tags":[]},{"title":"标题带图片的文章","slug":"带图片的文章 - 副本","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-21T01:47:12.709Z","comments":true,"path":"2013/07/13/带图片的文章 - 副本/","link":"","permalink":"http://example.com/2013/07/13/%E5%B8%A6%E5%9B%BE%E7%89%87%E7%9A%84%E6%96%87%E7%AB%A0%20-%20%E5%89%AF%E6%9C%AC/","excerpt":"","text":"11111111111111111111111111","categories":[],"tags":[]},{"title":"标题带图片的文章","slug":"带图片的文章","date":"2013-07-13T12:46:25.000Z","updated":"2022-06-21T01:47:12.709Z","comments":true,"path":"2013/07/13/带图片的文章/","link":"","permalink":"http://example.com/2013/07/13/%E5%B8%A6%E5%9B%BE%E7%89%87%E7%9A%84%E6%96%87%E7%AB%A0/","excerpt":"","text":"11111111111111111111111111","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"java","slug":"java","permalink":"http://example.com/tags/java/"},{"name":"消息队列","slug":"消息队列","permalink":"http://example.com/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://example.com/tags/Zookeeper/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"}]}